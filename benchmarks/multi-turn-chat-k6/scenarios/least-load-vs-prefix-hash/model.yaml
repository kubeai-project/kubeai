apiVersion: kubeai.org/v1
kind: Model
metadata:
  name: llama-3.1-8b-instruct-fp8-l4
spec:
  features: [TextGeneration]
  url: hf://neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8
  engine: VLLM
  args:
    - --enable-prefix-caching
    - --max-model-len=16384
    - --max-num-batched-token=16384
    - --gpu-memory-utilization=0.6
    - --disable-log-requests
  resourceProfile: nvidia-gpu-l4:1
  minReplicas: 2
  maxReplicas: 2
