{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"KubeAI: AI Inferencing Operator","text":"<p>Deploy and scale machine learning models on Kubernetes. Built for LLMs, embeddings, and speech-to-text.</p>"},{"location":"#highlights","title":"Highlights","text":"<p>What is it for?</p> <p>\ud83d\ude80 LLM Inferencing - Operate vLLM and Ollama servers \ud83c\udf99\ufe0f Speech Processing - Transcribe audio with FasterWhisper \ud83d\udd22 Vector Embeddings - Generate embeddings with Infinity \ud83d\udcda Reranking - Reorder search results with cross-encoder models  </p> <p>What do you get?</p> <p>\u26a1\ufe0f Intelligent Scaling - Scale from zero to meet demand \ud83d\udcca Optimized Routing - Dramatically improves performance at scale (see paper) \ud83d\udcbe Model Caching - Automates downloading &amp; mounting (EFS, etc.) \ud83e\udde9 Dynamic Adapters - Orchestrates LoRA adapters across replicas \ud83d\udce8 Event Streaming - Integrates with Kafka, PubSub, and more  </p> <p>We strive for an \"it justs works\" experience:</p> <p>\ud83d\udd17 OpenAI Compatible - Works with OpenAI client libraries \ud83d\udee0\ufe0f Zero Dependencies - Does not require Istio, Knative, etc. \ud83d\udda5 Hardware Flexible - Runs on CPU, GPU, or TPU  </p> <p>Quotes from the community:</p> <p>reusable, well abstracted solution to run LLMs - Mike Ensor, Google</p>"},{"location":"#why-kubeai","title":"Why KubeAI?","text":""},{"location":"#better-performance-at-scale","title":"Better performance at scale","text":"<p>When running multiple replicas of vLLM, the random load balancing strategy built into kube-proxy that backs standard Kubernetes Services performs poorly (TTFT &amp; throughput). This is because vLLM isn't stateless, its performance is heavily influenced by the state of its KV cache.</p> <p>The KubeAI proxy includes a prefix-aware load balancing strategy that optimizes KV cache utilization - resulting in dramatic improvements to overall system performance.</p> <p></p> <p>See the full paper for more details.</p>"},{"location":"#simplicity-and-ease-of-use","title":"Simplicity and ease of use","text":"<p>KubeAI does not depend on other systems like Istio &amp; Knative (for scale-from-zero), or the Prometheus metrics adapter (for autoscaling). This allows KubeAI to work out of the box in almost any Kubernetes cluster. Day-two operations is greatly simplified as well - don't worry about inter-project version and configuration mismatches.</p> <p>The project ships with a catalog of popular models, pre-configured for common GPU types. This means you can spend less time tweaking vLLM-specific flags. As we expand, we plan to build out an extensive model optimization pipeline that will ensure you get the most out of your hardware.</p>"},{"location":"#openai-api-compatibility","title":"OpenAI API Compatibility","text":"<p>No need to change your client libraries, KubeAI supports the following endpoints:</p> <pre><code>/v1/chat/completions\n/v1/completions\n/v1/embeddings\n/v1/rerank\n/v1/models\n/v1/audio/transcriptions\n</code></pre>"},{"location":"#architecture","title":"Architecture","text":"<p>KubeAI consists of two primary sub-components:</p> <p>1. The model proxy: the KubeAI proxy provides an OpenAI-compatible API. Behind this API, the proxy implements a prefix-aware load balancing strategy that optimizes for KV the cache utilization of the backend serving engines (i.e. vLLM). The proxy also implements request queueing (while the system scales from zero replicas) and request retries (to seamlessly handle bad backends).</p> <p>2. The model operator: the KubeAI model operator manages backend server Pods directly. It automates common operations such as downloading models, mounting volumes, and loading dynamic LoRA adapters via the KubeAI Model CRD.</p> <p>Both of these components are co-located in the same deployment, but could be deployed independently.</p> <p></p>"},{"location":"#adopters","title":"Adopters","text":"<p>List of known adopters:</p> Name Description Link Telescope Telescope uses KubeAI for multi-region large scale batch LLM inference. trytelescope.ai Google Cloud Distributed Edge KubeAI is included as a reference architecture for inferencing at the edge. LinkedIn, GitLab Lambda You can try KubeAI on the Lambda AI Developer Cloud. See Lambda's tutorial and video. Lambda Vultr KubeAI can be deployed on Vultr Managed Kubernetes using the application marketplace. Vultr Arcee Arcee uses KubeAI for multi-region, multi-tenant SLM inference. Arcee Seeweb Seeweb leverages KubeAI for direct and client-facing GPU inference workloads. KubeAI can be deployed on any GPU server and SKS Seeweb <p>If you are using KubeAI and would like to be listed as an adopter, please make a PR.</p>"},{"location":"#local-quickstart","title":"Local Quickstart","text":"<p>Create a local cluster using kind or minikube.</p> TIP: If you are using Podman for kind... Make sure your Podman machine can use up to 6G of memory (by default it is capped at 2G):  <pre><code># You might need to stop and remove the existing machine:\npodman machine stop\npodman machine rm\n\n# Init and start a new machine:\npodman machine init --memory 6144 --disk-size 120\npodman machine start\n</code></pre> <pre><code>kind create cluster # OR: minikube start\n</code></pre> <p>Add the KubeAI Helm repository.</p> <pre><code>helm repo add kubeai https://www.kubeai.org\nhelm repo update\n</code></pre> <p>Install KubeAI and wait for all components to be ready (may take a minute).</p> <pre><code>helm install kubeai kubeai/kubeai --wait --timeout 10m\n</code></pre> <p>Install some predefined models.</p> <pre><code>cat &lt;&lt;EOF &gt; kubeai-models.yaml\ncatalog:\n  deepseek-r1-1.5b-cpu:\n    enabled: true\n    features: [TextGeneration]\n    url: 'ollama://deepseek-r1:1.5b'\n    engine: OLlama\n    minReplicas: 1\n    resourceProfile: 'cpu:1'\n  qwen2-500m-cpu:\n    enabled: true\n  nomic-embed-text-cpu:\n    enabled: true\nEOF\n\nhelm install kubeai-models kubeai/models \\\n    -f ./kubeai-models.yaml\n</code></pre> <p>Before progressing to the next steps, start a watch on Pods in a standalone terminal to see how KubeAI deploys models. </p> <pre><code>kubectl get pods --watch\n</code></pre>"},{"location":"#interact-with-deepseek-r1-15b","title":"Interact with Deepseek R1 1.5b","text":"<p>Because we set <code>minReplicas: 1</code> for the Deepseek model you should see a model Pod already coming up.</p> <p>Start a local port-forward to the bundled chat UI.</p> <pre><code>kubectl port-forward svc/open-webui 8000:80\n</code></pre> <p>Now open your browser to localhost:8000 and select the Deepseek model to start chatting with.</p>"},{"location":"#scale-up-qwen2-from-zero","title":"Scale up Qwen2 from Zero","text":"<p>If you go back to the browser and start a chat with Qwen2, you will notice that it will take a while to respond at first. This is because we set <code>minReplicas: 0</code> for this model and KubeAI needs to spin up a new Pod (you can verify with <code>kubectl get models -oyaml qwen2-500m-cpu</code>).</p>"},{"location":"#get-plugged-in","title":"Get Plugged-In","text":"<p>Read about concepts, guides, and API documentation on kubeai.org.</p> <p>\ud83c\udf1f Don't forget to drop us a star on GitHub and follow the repo to stay up to date!</p> <p></p> <p>Let us know about features you are interested in seeing or reach out with questions. Visit our Discord channel to join the discussion!</p> <p>Or just reach out on LinkedIn if you want to connect:</p> <ul> <li>Nick Stogner</li> <li>Sam Stoelinga</li> </ul>"},{"location":"benchmarks/llama-3.2-11b-vision/","title":"Llama 3.2 11B Vision Instruct vLLM Benchmarks","text":"<p>Single L4 GPU vLLM 0.6.2 <pre><code>python3 benchmark_serving.py --backend openai \\\n    --base-url http://localhost:8000/openai \\\n    --dataset-name=sharegpt --dataset-path=ShareGPT_V3_unfiltered_cleaned_split.json \\\n    --model meta-llama-3.2-11b-vision-instruct \\\n    --seed 12345 --tokenizer neuralmagic/Llama-3.2-11B-Vision-Instruct-FP8-dynamic\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  681.93\nTotal input tokens:                      230969\nTotal generated tokens:                  194523\nRequest throughput (req/s):              1.47\nOutput token throughput (tok/s):         285.25\nTotal Token throughput (tok/s):          623.95\n---------------Time to First Token----------------\nMean TTFT (ms):                          319146.12\nMedian TTFT (ms):                        322707.98\nP99 TTFT (ms):                           642512.79\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          54.84\nMedian TPOT (ms):                        53.66\nP99 TPOT (ms):                           83.75\n---------------Inter-token Latency----------------\nMean ITL (ms):                           54.09\nMedian ITL (ms):                         47.44\nP99 ITL (ms):                            216.77\n==================================================\n</code></pre></p>"},{"location":"benchmarks/prefix-aware-load-balancing/","title":"Benchmarking Prefix Aware Load Balancing","text":"<p>Prefix Aware Load Balancing is able to improve throughput, inter-token latency and Time To First Token (TTFT). Even under heavy load the Time to First Token stays stable.</p> <p>The benchmarks demonstrate the following improvements by enabling Prefix Aware Load Balancing under heavy load (8000 concurrent requests):</p> <ul> <li>164x improvement in Mean TTFT from 39163.80 ms to 237.06 ms.</li> <li>41% increased throughput (total token/second) from 47609.83 to 67333.71.</li> <li>143% improvement in inter-token latency from 194.44 ms 79.90 ms.</li> </ul> <p>The benchmarks compare the following load balancing strategies: * Kubernetes Native Service - Round Robin. Randomly distribute requests across all instances. * KubeAI - Least Load. Send the request to the instance that is handling the least amount of requests. * KubeAI - Prefix Hash. Send the request to an instance that has already handled the same prefix or portial prefix.</p> <p>The benchmarks were run on 8 instances of vLLM serving LLama 3.1 8B and each uses a single L4 GPU. vLLM was configured to utilize prefix caching.</p> <p>The ShareGPT dataset was purposely crafted such that prompts shared partial prefixes. See more about dataset in the dataset section. Performance gains are lower when the amount of partial prefix re-use is less.</p> <p>Comparing the Mean TTFT for each Load Balancing strategy.</p> <p></p> <p>We can see that as the engine becomes overloaded, the Mean TTFT increases significantly for both K8s Native service and KubeAI Least Load. However, KubeAI Prefix Aware Load Balancing is able to achieve a stable TTFT even under heavy load.</p> <p>Comparing the throughput in tokens per second for each load balancing strategy:</p> <p></p> <p>The graph shows that even at low load you can get a significant improvement in throughput by enabling Prefix Aware Load Balancing.</p> <p>Conclusion: Prefix Aware Load Balancing is a must-have for large scale inference workloads.</p>"},{"location":"benchmarks/prefix-aware-load-balancing/#dataset-and-benchmarking-script","title":"Dataset and Benchmarking script","text":"<p>Dataset: ShareGPT filtered to only include conversations of 16 messages or more. This helps to simulate a scenario where people ask follow up questions and increases the number of partial prefix re-use.</p> <p>The vLLM benchmark_serving.py script was used for this but with a few  modifications: * Removed the limit to only include 2 messages per conversation * Create multiple prompts from a single conversation. E.g. prompt 1 would include message (1) of conversation x and prompt 2 would include message  (1, 2, 3) of conversation x. This resembles multi-round conversation of ChatGPT. * added <code>--max-conversations</code> parameter which limits of unique conversations to use.</p> <p>The script can be found under <code>kubeai/benchmarks/chat-py/benchmark_serving.py</code>.</p> <p>The image that was used: <code>substratusai/benchmark_serving:v0.0.1</code></p>"},{"location":"benchmarks/prefix-aware-load-balancing/#benchmarking-setup","title":"Benchmarking Setup","text":"<ul> <li>Scale: 8 instances of vLLM</li> <li>GPU: L4 GPU, 1 per instance</li> <li>Model: Llama 3.1 8B Instruct FP8</li> </ul> <p>The following model was deployed in KubeAI:</p> <pre><code>apiVersion: kubeai.org/v1\nkind: Model\nmetadata:\n  name: llama-3.1-8b-instruct-fp8-l4\nspec:\n  features: [TextGeneration]\n  url: hf://neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8\n  engine: VLLM\n  env:\n    VLLM_USE_V1: \"1\"\n  args:\n  - --enable-prefix-caching\n  - --max-model-len=16384\n  - --max-num-batched-token=16384\n  - --gpu-memory-utilization=0.95\n  - --disable-log-requests\n  - --kv-cache-dtype=fp8\n  resourceProfile: nvidia-gpu-l4:1\n  minReplicas: 8\n  maxReplicas: 8\n</code></pre> <p>In order to test Prefix Aware Load Balancing, we modify the load balancing strategy on the model object itself:</p> <p><pre><code>kubectl patch model llama-3.1-8b-instruct-fp8-l4 --type='merge' \\\n  -p '{\"spec\": {\"loadBalancing\": {\"strategy\": \"PrefixHash\"}}}'\n</code></pre> This takes effect right away and does not require recreation of the pods.</p> <p>K8s native service was tested by sending requests directly to the K8s service instead of the KubeAI proxy/LB. This allows us to test the default K8s service Round Robin based load balancing.</p> <p>This was the K8s Service used: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: vllm-direct\n  labels:\n    app: vllm-direct\nspec:\n  selector:\n    app.kubernetes.io/name: vllm\n  ports:\n    - name: http\n      protocol: TCP\n      port: 80\n      targetPort: 8000\n  type: ClusterIP\n</code></pre></p>"},{"location":"benchmarks/prefix-aware-load-balancing/#800-concurrent-requests","title":"800 concurrent requests","text":"<pre><code>      containers:\n        - name: benchmark-serving\n          image: substratusai/benchmark_serving:v0.0.1\n          args:\n            - --base-url=http://kubeai/openai\n            - --dataset-name=sharegpt\n            - --dataset-path=/app/sharegpt_16_messages_or_more.json\n            - --model=llama-3.1-8b-instruct-fp8-l4\n            - --seed=12345\n            - --tokenizer=neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8\n            - --request-rate=800\n            - --max-concurrency=800\n            - --num-prompts=8000\n            - --max-conversations=800\n      restartPolicy: Never\n</code></pre>"},{"location":"benchmarks/prefix-aware-load-balancing/#k8s-service-round-robin-no-kubeai-proxy","title":"K8s Service - Round Robin (No KubeAI proxy)","text":"<pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     8000      \nBenchmark duration (s):                  159.98    \nTotal input tokens:                      6656338   \nTotal generated tokens:                  608447    \nRequest throughput (req/s):              50.01     \nOutput token throughput (tok/s):         3803.20   \nTotal Token throughput (tok/s):          45409.81  \n---------------Time to First Token----------------\nMean TTFT (ms):                          1319.77   \nMedian TTFT (ms):                        601.29    \nP99 TTFT (ms):                           7438.41   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          189.29    \nMedian TPOT (ms):                        184.76    \nP99 TPOT (ms):                           486.16    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           173.06    \nMedian ITL (ms):                         94.60     \nP99 ITL (ms):                            715.66    \n==================================================\n</code></pre>"},{"location":"benchmarks/prefix-aware-load-balancing/#kubeai-least-load","title":"KubeAI - Least Load","text":"<pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     8000      \nBenchmark duration (s):                  158.39    \nTotal input tokens:                      6656338   \nTotal generated tokens:                  608447    \nRequest throughput (req/s):              50.51     \nOutput token throughput (tok/s):         3841.42   \nTotal Token throughput (tok/s):          45866.16  \n---------------Time to First Token----------------\nMean TTFT (ms):                          817.18    \nMedian TTFT (ms):                        494.28    \nP99 TTFT (ms):                           5551.81   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          191.44    \nMedian TPOT (ms):                        183.18    \nP99 TPOT (ms):                           520.48    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           176.03    \nMedian ITL (ms):                         124.55    \nP99 ITL (ms):                            691.97    \n==================================================\n</code></pre>"},{"location":"benchmarks/prefix-aware-load-balancing/#kubeai-prefix-hash","title":"KubeAI - Prefix Hash","text":"<pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     8000      \nBenchmark duration (s):                  104.67    \nTotal input tokens:                      6656338   \nTotal generated tokens:                  608447    \nRequest throughput (req/s):              76.43     \nOutput token throughput (tok/s):         5813.11   \nTotal Token throughput (tok/s):          69407.79  \n---------------Time to First Token----------------\nMean TTFT (ms):                          280.20    \nMedian TTFT (ms):                        239.80    \nP99 TTFT (ms):                           1260.53   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          86.55     \nMedian TPOT (ms):                        91.13     \nP99 TPOT (ms):                           139.47    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           85.78     \nMedian ITL (ms):                         77.35     \nP99 ITL (ms):                            272.04    \n==================================================\n</code></pre>"},{"location":"benchmarks/prefix-aware-load-balancing/#1600-concurrent-requests","title":"1600 concurrent requests","text":"<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: benchmark-serving\nspec:\n  template:\n    spec:\n      containers:\n        - name: benchmark-serving\n          image: substratusai/benchmark_serving:v0.0.1\n          args:\n            - --base-url=http://kubeai/openai\n            - --dataset-name=sharegpt\n            - --dataset-path=/app/sharegpt_16_messages_or_more.json\n            - --model=llama-3.1-8b-instruct-fp8-l4\n            - --seed=12345\n            - --tokenizer=neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8\n            - --request-rate=200\n            - --max-concurrency=1600\n            - --num-prompts=8000\n            - --max-conversations=800\n      restartPolicy: Never\n</code></pre>"},{"location":"benchmarks/prefix-aware-load-balancing/#k8s-service-round-robin","title":"K8s Service - Round Robin","text":"<pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     8000      \nBenchmark duration (s):                  157.07    \nTotal input tokens:                      6656338   \nTotal generated tokens:                  608447    \nRequest throughput (req/s):              50.93     \nOutput token throughput (tok/s):         3873.62   \nTotal Token throughput (tok/s):          46250.51  \n---------------Time to First Token----------------\nMean TTFT (ms):                          10365.29  \nMedian TTFT (ms):                        10068.73  \nP99 TTFT (ms):                           22283.86  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          216.53    \nMedian TPOT (ms):                        207.58    \nP99 TPOT (ms):                           607.73    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           197.37    \nMedian ITL (ms):                         90.35     \nP99 ITL (ms):                            749.96    \n==================================================\n</code></pre>"},{"location":"benchmarks/prefix-aware-load-balancing/#kubeai-least-load_1","title":"KubeAI - Least Load","text":"<pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     8000      \nBenchmark duration (s):                  153.02    \nTotal input tokens:                      6656338   \nTotal generated tokens:                  608447    \nRequest throughput (req/s):              52.28     \nOutput token throughput (tok/s):         3976.28   \nTotal Token throughput (tok/s):          47476.29  \n---------------Time to First Token----------------\nMean TTFT (ms):                          10579.01  \nMedian TTFT (ms):                        11501.96  \nP99 TTFT (ms):                           15514.10  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          212.39    \nMedian TPOT (ms):                        202.98    \nP99 TPOT (ms):                           613.06    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           193.34    \nMedian ITL (ms):                         92.65     \nP99 ITL (ms):                            747.65    \n==================================================\n</code></pre>"},{"location":"benchmarks/prefix-aware-load-balancing/#kubeai-prefix-hash_1","title":"KubeAI - Prefix Hash","text":"<pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     8000      \nBenchmark duration (s):                  110.00    \nTotal input tokens:                      6656338   \nTotal generated tokens:                  608447    \nRequest throughput (req/s):              72.73     \nOutput token throughput (tok/s):         5531.31   \nTotal Token throughput (tok/s):          66043.15  \n---------------Time to First Token----------------\nMean TTFT (ms):                          196.13    \nMedian TTFT (ms):                        184.29    \nP99 TTFT (ms):                           492.33    \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          78.51     \nMedian TPOT (ms):                        81.50     \nP99 TPOT (ms):                           117.36    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           79.20     \nMedian ITL (ms):                         70.36     \nP99 ITL (ms):                            249.71    \n==================================================\n</code></pre>"},{"location":"benchmarks/prefix-aware-load-balancing/#3200-concurrent-requests","title":"3200 concurrent requests","text":"<p>job: <pre><code>        - name: benchmark-serving\n          image: substratusai/benchmark_serving:v0.0.1\n          args:\n            - --base-url=http://kubeai/openai\n            - --dataset-name=sharegpt\n            - --dataset-path=/app/sharegpt_16_messages_or_more.json\n            - --model=llama-3.1-8b-instruct-fp8-l4\n            - --seed=12345\n            - --tokenizer=neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8\n            - --request-rate=200\n            - --max-concurrency=3200\n            - --num-prompts=8000\n            - --max-conversations=800\n</code></pre></p>"},{"location":"benchmarks/prefix-aware-load-balancing/#k8s-native-round-robin","title":"K8s Native - Round Robin","text":"<pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     8000      \nBenchmark duration (s):                  156.36    \nTotal input tokens:                      6656338   \nTotal generated tokens:                  608447    \nRequest throughput (req/s):              51.16     \nOutput token throughput (tok/s):         3891.22   \nTotal Token throughput (tok/s):          46460.74  \n---------------Time to First Token----------------\nMean TTFT (ms):                          27183.41  \nMedian TTFT (ms):                        31260.66  \nP99 TTFT (ms):                           51797.57  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          214.63    \nMedian TPOT (ms):                        205.61    \nP99 TPOT (ms):                           629.95    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           195.30    \nMedian ITL (ms):                         88.07     \nP99 ITL (ms):                            742.53    \n==================================================\n</code></pre>"},{"location":"benchmarks/prefix-aware-load-balancing/#kubeai-least-load_2","title":"KubeAI - Least Load","text":"<pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     8000      \nBenchmark duration (s):                  152.43    \nTotal input tokens:                      6656338   \nTotal generated tokens:                  608447    \nRequest throughput (req/s):              52.48     \nOutput token throughput (tok/s):         3991.56   \nTotal Token throughput (tok/s):          47658.74  \n---------------Time to First Token----------------\nMean TTFT (ms):                          24147.86  \nMedian TTFT (ms):                        25580.61  \nP99 TTFT (ms):                           46021.48  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          211.98    \nMedian TPOT (ms):                        201.97    \nP99 TPOT (ms):                           598.14    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           192.94    \nMedian ITL (ms):                         93.29     \nP99 ITL (ms):                            721.71    \n==================================================\n</code></pre>"},{"location":"benchmarks/prefix-aware-load-balancing/#kubeai-prefix-hash_2","title":"KubeAI - Prefix Hash","text":"<pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     8000      \nBenchmark duration (s):                  111.37    \nTotal input tokens:                      6656338   \nTotal generated tokens:                  608447    \nRequest throughput (req/s):              71.84     \nOutput token throughput (tok/s):         5463.50   \nTotal Token throughput (tok/s):          65233.60  \n---------------Time to First Token----------------\nMean TTFT (ms):                          213.92    \nMedian TTFT (ms):                        188.53    \nP99 TTFT (ms):                           838.35    \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          78.73     \nMedian TPOT (ms):                        82.17     \nP99 TPOT (ms):                           122.60    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           78.49     \nMedian ITL (ms):                         70.32     \nP99 ITL (ms):                            242.44    \n==================================================\n</code></pre>"},{"location":"benchmarks/prefix-aware-load-balancing/#concurrent-requests-8000","title":"concurrent requests 8000","text":"<pre><code>        - name: benchmark-serving\n          image: substratusai/benchmark_serving:v0.0.1\n          args:\n            - --base-url=http://kubeai/openai\n            - --dataset-name=sharegpt\n            - --dataset-path=/app/sharegpt_16_messages_or_more.json\n            - --model=llama-3.1-8b-instruct-fp8-l4\n            - --seed=12345\n            - --tokenizer=neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8\n            - --request-rate=800\n            - --max-concurrency=8000\n            - --num-prompts=8000\n            - --max-conversations=800\n</code></pre>"},{"location":"benchmarks/prefix-aware-load-balancing/#k8s-native-round-robin_1","title":"K8s Native - Round robin","text":"<pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     8000      \nBenchmark duration (s):                  156.20    \nTotal input tokens:                      6656338   \nTotal generated tokens:                  608447    \nRequest throughput (req/s):              51.22     \nOutput token throughput (tok/s):         3895.38   \nTotal Token throughput (tok/s):          46510.40  \n---------------Time to First Token----------------\nMean TTFT (ms):                          48587.55  \nMedian TTFT (ms):                        48682.53  \nP99 TTFT (ms):                           101940.11 \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          215.24    \nMedian TPOT (ms):                        206.65    \nP99 TPOT (ms):                           566.10    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           196.77    \nMedian ITL (ms):                         87.08     \nP99 ITL (ms):                            751.68    \n==================================================\n</code></pre>"},{"location":"benchmarks/prefix-aware-load-balancing/#kubeai-least-load_3","title":"KubeAI - Least Load","text":"<pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     8000      \nBenchmark duration (s):                  152.59    \nTotal input tokens:                      6656338   \nTotal generated tokens:                  608447    \nRequest throughput (req/s):              52.43     \nOutput token throughput (tok/s):         3987.46   \nTotal Token throughput (tok/s):          47609.83  \n---------------Time to First Token----------------\nMean TTFT (ms):                          39163.80  \nMedian TTFT (ms):                        40140.70  \nP99 TTFT (ms):                           78489.26  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          214.09    \nMedian TPOT (ms):                        205.62    \nP99 TPOT (ms):                           623.61    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           194.44    \nMedian ITL (ms):                         90.36     \nP99 ITL (ms):                            725.95    \n==================================================\n</code></pre>"},{"location":"benchmarks/prefix-aware-load-balancing/#kubeai-prefix-hash_3","title":"KubeAI - Prefix Hash","text":"<pre><code>============ Serving Benchmark Result ============\nSuccessful requests:                     8000      \nBenchmark duration (s):                  107.89    \nTotal input tokens:                      6656338   \nTotal generated tokens:                  608447    \nRequest throughput (req/s):              74.15     \nOutput token throughput (tok/s):         5639.40   \nTotal Token throughput (tok/s):          67333.71  \n---------------Time to First Token----------------\nMean TTFT (ms):                          237.06    \nMedian TTFT (ms):                        219.27    \nP99 TTFT (ms):                           619.65    \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          79.99     \nMedian TPOT (ms):                        81.76     \nP99 TPOT (ms):                           124.28    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           79.90     \nMedian ITL (ms):                         71.31     \nP99 ITL (ms):                            303.14    \n==================================================\n</code></pre>"},{"location":"blog/","title":"Recent","text":""},{"location":"blog/2025/02/26/llm-load-balancing-at-scale-chwbl/","title":"LLM Load Balancing at Scale: Consistent Hashing with Bounded Loads","text":"<p>Conducted by Substratus.AI, the creators of KubeAI.</p>"},{"location":"blog/2025/02/26/llm-load-balancing-at-scale-chwbl/#abstract","title":"Abstract:","text":"<p>Operating large language models (LLMs) at scale in real-world scenarios necessitates multiple backend replicas. The inter-replica load balancing algorithm greatly influences overall system performance by impacting the caching dynamics inherent in modern inference engines. In this study, we explore the application of the Consistent Hashing with Bounded Loads (CHWBL) algorithm in the domain of LLM inference. We simulate real-world load patterns on Kubernetes, the industry's most widely adopted deployment platform.  Our results demonstrate a 95% reduction in Time To First Token (TTFT) and a 127% increase in overall throughput compared to the baseline strategy.</p>"},{"location":"blog/2025/02/26/llm-load-balancing-at-scale-chwbl/#1-introduction","title":"1. Introduction","text":"<p>Before an inference engine such as vLLM can begin generating output tokens, it must first process the input prompt during the \u201cprefill phase\u201d. The resulting intermediate representation is stored in a key-value (KV) cache. This cache can be reused across multiple requests that share the same prefix - a feature known as \"prefix caching\". The impact of prefix caching can be significant in several real world scenarios:</p> <p>Multi-turn Conversations: In applications like chatbots (e.g., ChatGPT) or autonomous AI agents, every new turn appends to an evolving shared context.</p> <p></p> <p>Multi-threaded Requests with Shared Context: Scenarios that involve multiple queries against a single long document are particularly sensitive to the efficiency of prefix caching. When requests are executed concurrently, even slight improvements in cache hit rates can lead to substantial reductions in end-to-end latency.</p> <p></p> <p>The conventional random routing strategy provided by Kubernetes often results in suboptimal cache utilization, leading to frequent cache evictions and degraded performance.</p> <p></p>"},{"location":"blog/2025/02/26/llm-load-balancing-at-scale-chwbl/#2-problem-statement","title":"2. Problem Statement","text":"<p>An effective load balancing strategy for LLM serving should satisfy the following criteria:</p> <p>Maximize cache utilization: Route requests with common prefixes to vLLM replicas with hot caches.</p> <p>Adapt to replica changes: Minimize cache shuffling as replicas come and go.</p> <p>Avoid hotspots: Prevent any single replica from becoming overloaded.</p> <p>Account for LoRA adapters: Prefix caches for different adapters should be considered distinct.</p> <p>Avoid client-side requirements: Common client libraries should be usable out of the box.</p>"},{"location":"blog/2025/02/26/llm-load-balancing-at-scale-chwbl/#3-sticky-sessions","title":"3. Sticky Sessions?","text":"<p>Load balancing across stateful backends is not a new problem. We considered sticky sessions, the go-to for stateful load balancing. In a ChatGPT-like scenario, cookies could be used to reliably map users to active threads. However many agentic systems are not browser based, leaving client IP as the primary out-of-the-box source for client identification. Client-IP is typically unreliable for determining the end-client (NAT, dynamic IPs). To make matters worse, a single agentic program typically simulates N logical agents - i.e. multiple concurrent multi-turn conversations.</p>"},{"location":"blog/2025/02/26/llm-load-balancing-at-scale-chwbl/#4-the-algorithm","title":"4. The Algorithm","text":"<p>The Consistent Hashing with Bounded Loads (CHWBL) algorithm inherently addresses the above challenges, making it a compelling choice for LLM load balancing. It has a proven track record being employed at-scale in cache-sensitive use cases such as video serving.</p> <p>The CHWBL algorithm extends traditional consistent hashing by incorporating load bounds, ensuring that no individual replica receives more than its fair share of requests. This approach not only preserves cache affinity but also prevents the overloading of any single server, thereby optimizing overall system performance.</p> <p></p>"},{"location":"blog/2025/02/26/llm-load-balancing-at-scale-chwbl/#5-implementation","title":"5. Implementation","text":"<p>We integrated the CHWBL routing strategy into the KubeAI project under the PrefixHash configuration. This strategy functions as follows:</p> <ol> <li>Request Inspection: The incoming HTTP body is analyzed (OpenAI-formatted API request).</li> <li>Prefix Extraction: A prefix is extracted from the request (for example, using the first user message in chat completions) as well as the requested LoRA adapter (if applicable).</li> <li>Hashing: The <code>(extracted_prefix + lora_adapter_name)</code> is hashed using the <code>xxHash</code> algorithm.</li> <li>Replica Lookup: The hash value is used to select the appropriate vLLM replica using the CHWBL algorithm. The configuration is specified on a per-model basis via the following YAML snippet:</li> </ol> <pre><code>kind: Model\nspec:\n  # ...\n  loadBalancing:\n    strategy: PrefixHash\n    # Optional parameters:\n    prefixHash:\n      meanLoadFactor: 125\n      prefixCharLength: 100\n      replication: 256\n</code></pre>"},{"location":"blog/2025/02/26/llm-load-balancing-at-scale-chwbl/#6-evaluation","title":"6. Evaluation","text":""},{"location":"blog/2025/02/26/llm-load-balancing-at-scale-chwbl/#61-scenarios","title":"6.1. Scenarios","text":"<p>We conducted experiments using three distinct load balancing strategies:</p> <ol> <li>Kubernetes Service (Random)<ul> <li>Utilized the default Kubernetes Service, bypassing the KubeAI proxy.</li> <li>Relied on <code>iptables</code> for proxying via kube-proxy.</li> </ul> </li> <li>KubeAI (LeastLoad)<ul> <li>Employed the KubeAI load balancer to route traffic to the replica with the fewest active requests.</li> </ul> </li> <li>KubeAI (PrefixHash)<ul> <li>Applied a CHWBL-driven strategy to route requests via the KubeAI proxy.</li> </ul> </li> </ol>"},{"location":"blog/2025/02/26/llm-load-balancing-at-scale-chwbl/#62-setup","title":"6.2. Setup","text":"<p>A Kubernetes cluster running KubeAI was setup with the following specifications:</p> <ul> <li>Hardware: 8x L4 GPUs</li> <li>Software: 8x vLLM instances</li> <li>Model: Llama 3.1 8B</li> <li>Dataset: Message threads derived from ShareGPT</li> <li>Workload: A custom load generator simulating parallel chat completion threads, preserving conversation state by appending LLM responses in a loop.</li> </ul>"},{"location":"blog/2025/02/26/llm-load-balancing-at-scale-chwbl/#63-metrics","title":"6.3. Metrics","text":"<p>Our evaluation focused on two key performance metrics:</p> <ul> <li>Time To First Token (TTFT): The latency before the first token is generated.</li> <li>Tokens Per Second (TPS): The overall throughput of token generation.</li> </ul>"},{"location":"blog/2025/02/26/llm-load-balancing-at-scale-chwbl/#64-results","title":"6.4. Results","text":"<p>The benchmark showed that the PrefixHash strategy resulted in significant improvements in TTFT and throughput compared to both the default Kubernetes Service and the LeastLoad strategy.</p> <p>TTFT: The PrefixHash strategy resulted in a 95% reduction in Time to First Token compared to the built-in Kubernetes strategy when the system was operating at 1200 concurrent threads (150 threads per replica).</p> <p></p> <p>Throughput: The PrefixHash strategy resulted in a 127% increase in throughput compared to the built-in Kubernetes strategy when the system was operating at 1200 concurrent threads (150 threads per replica).</p> <p></p>"},{"location":"blog/2025/02/26/llm-load-balancing-at-scale-chwbl/#7-conclusion","title":"7. Conclusion","text":"<p>In this work, we demonstrated that incorporating the CHWBL algorithm into LLM load balancing through the PrefixHash strategy in KubeAI can significantly enhance performance in real-world scenarios. Notably, improvements in cache hit rates yielded lower latency and increased throughput. Moving forward, we plan to expand our benchmarks to cover a broader range of workloads. We plan to incorporate benchmarks for emerging strategies as they come out, with the ultimate aim of ensuring that KubeAI features the most effective load balancing techniques.</p>"},{"location":"blog/2025/02/26/llm-load-balancing-at-scale-chwbl/#references","title":"References","text":"<ul> <li>vLLM Documentation on Automatic Prefix Caching</li> <li>Google Research: Consistent Hashing with Bounded Loads.</li> <li>KubeAI Load Balancing Concepts</li> </ul>"},{"location":"concepts/autoscaling/","title":"Autoscaling","text":"<p>KubeAI proxies HTTP and messaging (i.e. Kafka, etc) requests and messages to models. It will adjust the number Pods serving a given model based on the average active number of requests. If no Pods are running when a request comes in, KubeAI will hold the request, scale up a Pod and forward the request when the Pod is ready. This process happens in a manner that is transparent to the end client (other than the added delay from a cold-start).</p> <p> </p>"},{"location":"concepts/autoscaling/#next","title":"Next","text":"<p>Read about how to configure autoscaling.</p>"},{"location":"concepts/backend-servers/","title":"Backend Servers","text":"<p>KubeAI serves ML models by launching Pods on Kubernetes. The configuration and lifecycle of these Pods are managed by the KubeAI controller. Every model server Pod loads exactly one model on startup.</p> <p>In a Model manifest you can define what server to use for inference (<code>VLLM</code>, <code>OLlama</code>). Any model-specific settings can be passed to the server process via the <code>args</code> and <code>env</code> fields.</p>"},{"location":"concepts/backend-servers/#next","title":"Next","text":"<p>Read about how to install models.</p>"},{"location":"concepts/load-balancing/","title":"Load Balancing","text":"<p>To optimize inference performance and resource utilization, KubeAI supports load balancing strategies specifically tailored for model inference servers such as vLLM. This document explains two primary load balancing strategies available in KubeAI: Least Load and Prefix Hash.</p>"},{"location":"concepts/load-balancing/#least-load","title":"Least Load","text":"<p>The Least Load strategy distributes inference requests to the model replica that has the least number of in-flight requests. This strategy aims to balance the inference workload evenly across available replicas, reducing the risk of overloading any single server.</p>"},{"location":"concepts/load-balancing/#prefix-hash","title":"Prefix Hash","text":"<p>The Prefix Hash strategy leverages the Consistent Hashing with With Bounded Loads (CHWBL) algorithm to optimize the performance of engines such as vLLM that support prefix caching. This strategy increases the likelihood of KV cache hits for common prefixes. See vLLM prefix hashing docs for more info.</p> <p>With this strategy, KubeAI hashes incoming requests based on their prefixes (in addition to a requested LoRA adapter name - if present). Requests with the same hash value are routed to the same replica, except when that replica's in-flight requests exceed the overall average by a configurable percentage.</p> <p>This strategy has the most benefit for use cases such as chat completion. This is because the entire chat thread is sent in each successive chat requests.</p> <p>KubeAI supports this strategy for the following endpoints:</p> <pre><code>/openai/v1/completions\n/openai/v1/chat/completions\n</code></pre>"},{"location":"concepts/load-balancing/#next","title":"Next","text":"<p>See the Kubernetes API docs to view how to configure Model load balancing.</p>"},{"location":"concepts/lora-adapters/","title":"LoRA Adapters","text":"<p>KubeAI orchestrates the loading of LoRA adapters into model serving containers. New LoRA adapters can be swapped in and out without needing to restart the container that is serving the base model.</p> <p></p>"},{"location":"concepts/lora-adapters/#next","title":"Next","text":"<p>Read about how to serve lora adapters.</p>"},{"location":"concepts/resource-profiles/","title":"Resource Profiles","text":"<p>A resource profile maps a type of compute resource (i.e. NVIDIA L4 GPU) to a collection of Kubernetes settings that are configured on inference server Pods. These profiles are defined in the KubeAI <code>config.yaml</code> file (via a ConfigMap). Each model specifies the resource profile that it requires.</p> <p>Kubernetes Model resources specify a resource profile and the count of that resource that they require (for example <code>resourceProfile: nvidia-gpu-l4:2</code> - 2x L4 GPUs).</p> <p>A given profile might need to contain slightly different settings based on the cluster/cloud that KubeAI is deployed in.</p> <p>Example: A resource profile named <code>nvidia-gpu-l4</code> might contain the following node selectors when installing KubeAI on a GKE Kubernetes cluster:</p> <pre><code>cloud.google.com/gke-accelerator: \"nvidia-l4\"\ncloud.google.com/gke-spot: \"true\"\n</code></pre> <p>and add the following resource requests to the model server Pods:</p> <pre><code>nvidia.com/gpu: \"1\"\n</code></pre> <p>In addition to node selectors and resource requirements, a resource profile may optionally specify an image name. This name maps to the container image that will be selected when serving a model on that resource.</p>"},{"location":"concepts/resource-profiles/#next","title":"Next","text":"<p>Read about how to configure resource profiles.</p>"},{"location":"concepts/storage-caching/","title":"Storage / Caching","text":"<p>With \"Large\" in the name, caching is a critical part of serving LLMs.</p> <p>The best caching technique may very depending on your environment:</p> <ul> <li>What cloud features are available?</li> <li>Is your cluster deployed in an air-gapped environment?</li> </ul>"},{"location":"concepts/storage-caching/#a-model-built-into-container","title":"A. Model built into container","text":"<p>Status: Supported</p> <p>Building a model into a container image can provide a simple way to take advantage of image-related optimizations built into Kubernetes:</p> <ul> <li> <p>Relaunching a model server on the same Node that it ran on before will likely be able to reuse the previously pulled image.</p> </li> <li> <p>Secondary boot disks on GKE can be used to avoid needing to pull images.</p> </li> <li> <p>Image streaming on GKE can allow for containers to startup before the entire image is present on the Node.</p> </li> <li> <p>Container images can be pre-installed on Nodes in air-gapped environments (example: k3s airgap installation).</p> </li> </ul> <p>Guides:</p> <ul> <li>How to build models into container images</li> </ul>"},{"location":"concepts/storage-caching/#b-model-on-shared-filesystem-read-write-many","title":"B. Model on shared filesystem (read-write-many)","text":"<p>KubeAI can manage model caches on a shared filesystem (i.e. AWS EFS, GCP Filestore, NFS). It manages the full lifecycle of a cached model: loading, serving, and cache eviction (on deletion of the Model).</p> <p> </p>"},{"location":"concepts/storage-caching/#c-model-on-read-only-many-disk","title":"C. Model on read-only-many disk","text":"<p>Status: Planned.</p> <p>Examples: GCP Hyperdisk ML</p>"},{"location":"contributing/development-environment/","title":"Development environment","text":"<p>This document provides instructions for setting up an environment for developing KubeAI.</p>"},{"location":"contributing/development-environment/#optional-cloud-setup","title":"Optional: Cloud Setup","text":""},{"location":"contributing/development-environment/#gcp-pubsub","title":"GCP PubSub","text":"<p>If you are develop PubSub messaging integration on GCP, setup test topics and subscriptions and uncomment the <code>.messaging.streams</code> in <code>./hack/dev-config.yaml</code>.</p> <pre><code>gcloud auth login --update-adc\n\ngcloud pubsub topics create test-kubeai-requests\ngcloud pubsub subscriptions create test-kubeai-requests-sub --topic test-kubeai-requests\ngcloud pubsub topics create test-kubeai-responses\ngcloud pubsub subscriptions create test-kubeai-responses-sub --topic test-kubeai-responses\n</code></pre>"},{"location":"contributing/development-environment/#run-in-local-cluster","title":"Run in Local Cluster","text":"<pre><code>kind create cluster\n# OR\n#./hack/create-dev-gke-cluster.yaml\n\n# Generate CRDs from Go code.\nmake generate &amp;&amp; make manifests\n\n# When CRDs are changed reapply using kubectl:\nkubectl apply -f ./charts/kubeai/templates/crds\n\n# Model with special address annotations:\nkubectl apply -f ./hack/dev-models/kind-cpu.yaml\n\n# OPTION A #\n# Run KubeAI inside cluster\n# Change `-f` based on the cluster environment.\nhelm upgrade --install kubeai ./charts/kubeai \\\n    --set open-webui.enabled=true \\\n    --set image.tag=latest \\\n    --set image.pullPolicy=Always \\\n    --set image.repository=us-central1-docker.pkg.dev/substratus-dev/default/kubeai \\\n    --set secrets.huggingface.token=$HUGGING_FACE_HUB_TOKEN \\\n    --set replicaCount=1 -f ./hack/dev-gke-helm-values.yaml\n\n# OPTION B #\n# For quick local interation (run KubeAI outside of cluster)\nkubectl create cm kubeai-autoscaler-state -oyaml --dry-run=client | kubectl apply -f -\nCONFIG_PATH=./hack/dev-configs/kind.yaml POD_NAMESPACE=default go run ./cmd/main.go\n\n# In another terminal:\nwhile true; do kubectl port-forward service/dev-model 7000:7000; done\n############\n</code></pre>"},{"location":"contributing/development-environment/#running","title":"Running","text":""},{"location":"contributing/development-environment/#completions-api","title":"Completions API","text":"<pre><code># If you are running kubeai in-cluster:\n# kubectl port-forward svc/kubeai 8000:80\n\ncurl http://localhost:8000/openai/v1/completions -H \"Content-Type: application/json\" -d '{\"prompt\": \"Hi\", \"model\": \"dev\"}' -v\n</code></pre>"},{"location":"contributing/development-environment/#messaging-integration","title":"Messaging Integration","text":"<pre><code>gcloud pubsub topics publish test-kubeai-requests \\                  \n  --message='{\"path\":\"/v1/completions\", \"metadata\":{\"a\":\"b\"}, \"body\": {\"model\": \"dev\", \"prompt\": \"hi\"}}'\n\ngcloud pubsub subscriptions pull test-kubeai-responses-sub --auto-ack\n</code></pre>"},{"location":"contributing/development-guide/","title":"KubeAI Development Guide","text":""},{"location":"contributing/development-guide/#openai-api","title":"OpenAI API","text":"<ul> <li>Types: See <code>./api/openai/v1/README.md</code></li> </ul>"},{"location":"contributing/development-guide/#build-and-run-commands","title":"Build and Run Commands","text":"<ul> <li>Build: <code>make build</code> (manager binary)</li> <li>Docker: <code>make docker-build</code></li> <li>Run locally: <code>make run</code></li> <li>Generate go code (for <code>./api/*</code>): <code>make generate</code></li> <li>Generate manifests: <code>make manifests</code></li> </ul>"},{"location":"contributing/development-guide/#testing-commands","title":"Testing Commands","text":"<ul> <li>Unit tests: <code>make test-unit</code></li> <li>Single unit test (does not work for integration tests): <code>go test -v ./path/to/package -run TestNamePattern</code></li> <li>Integration tests: <code>make test-integration RUN=SpecificTestToRun</code></li> <li>E2E tests: <code>make test-e2e-*</code> (various test suites)</li> <li>Must be run with an active <code>kind</code> cluster (Run <code>kind create cluster</code> if <code>kubectl config current-context</code> does not report a cluster as existing).</li> </ul>"},{"location":"contributing/development-guide/#code-style","title":"Code Style","text":"<ul> <li>Format: <code>make fmt</code> (standard Go formatting)</li> <li>Lint: <code>make lint</code> (golangci-lint v1.59.1)</li> <li>Vet: <code>make vet</code> (standard Go vetting)</li> </ul>"},{"location":"contributing/development-guide/#conventions","title":"Conventions","text":"<ul> <li>Standard Go project layout (cmd/, internal/, api/, test/)</li> <li>Table-driven tests with descriptive names</li> <li>Use testify for assertions</li> <li>Integration tests use require.EventuallyWithT for async verification</li> <li>Follow Kubernetes controller patterns (kubebuilder / controller-runtime)</li> </ul>"},{"location":"contributing/documentation/","title":"Documentation","text":"<p>We are grateful for anyone who takes the time to improve KubeAI documentation! In order to keep our docs clear and consistent we ask that you first read about the approach to documentation that we have standardized on...</p>"},{"location":"contributing/documentation/#read-before-writing","title":"Read before writing!","text":"<p>The KubeAI approach to documentation is loosely inspired by the Diataxis method.</p> <p>TLDR on how KubeAI docs are organized:</p> <ul> <li>Installation: How-to guides specific to installing KubeAI.</li> <li>How To: Directions that guide the reader through a problem or towards a result. How-to guides are goal-oriented. They assume the user is familiar with general concepts, tools, and has already installed KubeAI.</li> <li>Concepts: A reflective explanation of KubeAI topics with a focus on giving the reader an understanding of the why.</li> <li>Tutorials: Learning oriented experiences. Lessons that often guide a user from beginning to end. The goal is to help the reader learn something (compared to a how-to guide that is focused on helping the reader do something).</li> <li>Contributing: The docs in here differ from the rest of the docs by audience: these docs are for anyone who will be contributing code or docs to the KubeAI project.</li> </ul>"},{"location":"contributing/documentation/#how-to-serve-kubeaiorg-locally","title":"How to serve kubeai.org locally","text":"<p>Make sure you have python3 installed and run:</p> <pre><code>make docs\n</code></pre>"},{"location":"contributing/release-process/","title":"Release Process","text":"<p>This document describes the process for releasing a new version of the project.</p>"},{"location":"contributing/release-process/#docs","title":"Docs","text":"<p>The docs are automatically published whenever a PR updates the docs and the PR is merged into the main branch. The docs are published to the <code>gh-pages</code> branch, which is the source for the Github Pages site.</p>"},{"location":"contributing/release-process/#docker-images","title":"Docker images","text":"<p>The Docker image latest tag always points to the latest released version. The <code>main</code> tag points to the latest commit on the main branch.</p> <p>If you push a tag <code>vX.Y.Z</code> to the repository, the Docker image with the tag <code>vX.Y.Z</code> is built and pushed to Docker Hub. Afterwards, the <code>latest</code> tag is updated to point to the new version.</p> <p>Use the commands below to create a new release.</p> <p>Check the latest version tag:</p> <pre><code>git pull --tags\ngit tag -l | grep -E \"^v[0-9]\" | sort -t \".\" -k1,1n -k2,2n -k3,3n\n</code></pre> <p>This may show <code>v0.14.0</code>. If that's the case then tag the next version with <code>v0.15.0</code> by running:</p> <pre><code>git checkout main\ngit pull origin main\ngit tag v0.15.0\ngit push --tags\n</code></pre> <p>You can go to the GitHub action page and see that a new action has been triggered. This action to build and publish needs to succeed.</p> <p>Afterwards you can pull the new image:</p> <pre><code>docker pull substratusai/kubeai:v0.15.0\n</code></pre>"},{"location":"contributing/release-process/#helm-chart","title":"Helm Chart","text":"<p>The Helm chart only gets released when a git tag is pushed to the repository with the format <code>helm-v*</code>.</p> <p>The appVersion in the Helm chart does not have to point to the latest released version. This allows us to first publish a new version of the Docker image without updating the Helm chart. The Helm chart is updated when we are ready to release a new version.</p> <p>This is important when a new appVersion isn't compatible with the current Helm chart. In those cases, we can first merge the PR, thoroughly test, release new container image, and then in a separate PR update the Helm chart and the appVersion.</p> <p>Steps to release a new version: 1. Create a new PR and update the helm chart versions and appVersion. 2. Merge the PR into main. 3. Create a new tag from main branch with the format <code>helm-vX.Y.Z</code>. Make sure to match the tag to the helm chart version. Push the tag.</p> <p>After merging to main:</p> <pre><code>git pull origin main\ngit tag helm-v0.12.0\ngit push --tags\n</code></pre>"},{"location":"how-to/architect-for-multitenancy/","title":"Architect for multitenancy","text":"<p>KubeAI can support multitenancy by filtering the models that it serves via Kubernetes label selectors. These label selectors can be applied when accessing any of the OpenAI-compatible endpoints through the <code>X-Label-Selector</code> HTTP header and will match on labels specified on the <code>kind: Model</code> objects. The pattern is similar to using a <code>WHERE</code> clause in a SQL query.</p> <p>Example Models:</p> <pre><code>kind: Model\nmetadata:\n  name: llama-3.2\n  labels:\n    tenancy: public\nspec:\n# ...\n---\nkind: Model\nmetadata:\n  name: custom-private-model\n  labels:\n    tenancy: org-abc\nspec:\n# ...\n</code></pre> <p>Example Model using Helm chart: <pre><code>catalog:\n  llama-3.2:\n    labels:\n      tenancy: public\n    # ...\n</code></pre></p> <p>Example HTTP requests:</p> <pre><code># The returned list of models will be filtered.\ncurl http://$KUBEAI_ENDPOINT/openai/v1/models \\\n    -H \"X-Label-Selector: tenancy in (org-abc, public)\"\n\n# When running inference, if the label selector does not match\n# a 404 will be returned.\ncurl http://$KUBEAI_ENDPOINT/openai/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -H \"X-Label-Selector: tenancy in (org-abc, public)\" \\\n    -d '{\"prompt\": \"Hi\", \"model\": \"llama-3.2\"}'\n</code></pre> <p>The header value can be any valid Kubernetes label selector. Some examples include:</p> <pre><code>X-Label-Selector: tenancy=org-abc\nX-Label-Selector: tenancy in (org-abc, public)\nX-Label-Selector: tenancy!=private\n</code></pre> <p>Multiple <code>X-Label-Selector</code> headers can be specified in the same HTTP request and will be treated as a logical <code>AND</code>. For example, the following request will only match Models that have a label <code>tenant: org-abc</code> and <code>user: sam</code>:</p> <pre><code>curl http://$KUBEAI_ENDPOINT/openai/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -H \"X-Label-Selector: tenant=org-abc\" \\\n    -H \"X-Label-Selector: user=sam\" \\\n    -d '{\"prompt\": \"Hi\", \"model\": \"llama-3.2\"}'\n</code></pre> <p>Example architecture:</p> <p></p>"},{"location":"how-to/authenticate-to-model-repos/","title":"Authenticate to model repos","text":"<p>KubeAI supports the following private model repositories, and different authentication methods.</p>"},{"location":"how-to/authenticate-to-model-repos/#helm","title":"Helm","text":""},{"location":"how-to/authenticate-to-model-repos/#alibaba-object-storage-service","title":"Alibaba Object Storage Service","text":"<p>Example url: <code>oss://my-oss-bucket/my-models/llama-3.1-8b-instruct</code></p> <p>Authenication is required when accessing models or adapters from private OSS buckets.</p> <p>When using Helm to manage your KubeAI installation, you can pass your credentials as follows:</p> <pre><code>helm upgrade --install kubeai kubeai/kubeai \\\n    --set secrets.alibaba.accessKeyID=$OSS_ACCESS_KEY_ID \\\n    --set secrets.alibaba.accessKeySecret=$OSS_ACCESS_KEY_SECRET \\\n    ...\n</code></pre> <p>NOTE: KubeAI does not automatically react to updates to credentials. You will need to manually delete and allow KubeAI to recreate any failed Jobs/Pods that required credentials.</p>"},{"location":"how-to/authenticate-to-model-repos/#google-cloud-storage","title":"Google Cloud Storage","text":"<p>Example url: <code>gs://my-gcs-bucket/my-models/llama-3.1-8b-instruct</code></p> <p>Authenication is required when accessing models or adapters from private GCS buckets.</p> <p>When using Helm to manage your KubeAI installation, you can pass your credentials as follows:</p> <pre><code>helm upgrade --install kubeai kubeai/kubeai \\\n    --set secrets.gcp.jsonKeyfile=$MY_JSON_KEYFILE \\\n    ...\n</code></pre> <p>NOTE: KubeAI does not automatically react to updates to credentials. You will need to manually delete and allow KubeAI to recreate any failed Jobs/Pods that required credentials.</p>"},{"location":"how-to/authenticate-to-model-repos/#huggingface-hub","title":"HuggingFace Hub","text":"<p>Example model url: <code>hf://meta-llama/Llama-3.1-8B-Instruct</code></p> <p>Authentication is required when loading models or adapters from HuggingFace Hub when using a private Hub or accessing a public model that requires agreeing to terms of service.</p> <p>When using Helm to manage your KubeAI installation, you can pass your credentials as follows:</p> <pre><code>helm upgrade --install kubeai kubeai/kubeai \\\n    --set secrets.huggingface.token=$HUGGING_FACE_HUB_TOKEN \\\n    ...\n</code></pre> <p>NOTE: KubeAI does not automatically react to updates to credentials. You will need to manually delete and allow KubeAI to recreate any failed Jobs/Pods that required credentials.</p>"},{"location":"how-to/authenticate-to-model-repos/#s3","title":"S3","text":"<p>Example model url: <code>s3://my-private-model-bucket/my-models/llama-3.1-8b-instruct</code></p> <p>Authenication is required when accessing models or adapters from private S3 buckets.</p> <p>When using Helm to manage your KubeAI installation, you can pass your credentials as follows:</p> <pre><code>helm upgrade --install kubeai kubeai/kubeai \\\n    --set secrets.aws.accessKeyID=$AWS_ACCESS_KEY_ID \\\n    --set secrets.aws.secretAccessKey=$AWS_SECRET_ACCESS_KEY \\\n    ...\n</code></pre> <p>NOTE: KubeAI does not automatically react to updates to credentials. You will need to manually delete and allow KubeAI to recreate any failed Jobs/Pods that required credentials.</p>"},{"location":"how-to/authenticate-to-model-repos/#model-spec","title":"Model Spec","text":"<p>You can also pass credentials using envFrom in the model spec. This is an example how to confiure an S3 self managed instance with and envFrom.</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: set1\ntype: kubernetes.io/basic-auth\nstringData:\n  AWS_ACCESS_KEY_ID: test\n  AWS_SECRET_ACCESS_KEY: testtest\n  HF_TOKEN: secret\n---\napiVersion: kubeai.org/v1\nkind: Model\nmetadata:\n  name: llama-test-s3\nspec:\n  features: [TextGeneration]\n  owner: meta-llama\n  url: s3://models/Llama-3.2-1B\n  adapters: # &lt;--\n  - name: llama-adapter\n    url: s3://adapters/llama-3.1-8b-ocr-correction\n  engine: VLLM\n  env:\n    AWS_ENDPOINT_URL: http://locals3:9000\n  envFrom:\n    - secretRef:\n        name: set1\n</code></pre> <p>NOTE: If both configuration methods are used, the helm method takes precedence.</p>"},{"location":"how-to/build-models-into-containers/","title":"Build models into containers","text":"<p>In this guide we will preload a LLM into a custom built Ollama serving image. You can follow the same steps for other models and other serving engines.</p> <p>Define some values <pre><code>export MODEL_URL=ollama://qwen2:0.5b\n\n# Customize with your own image repo.\nexport IMAGE=us-central1-docker.pkg.dev/substratus-dev/default/ollama-builtin-qwen2-05b:latest\n</code></pre></p> <p>Build and push image. Note: building (downloading base image &amp; model) and pushing (uploading image &amp; model) can take a while depending on the size of the model.</p> <pre><code>git clone https://github.com/substratusai/kubeai\ncd ./kubeai/examples/ollama-builtin\n\ndocker build --build-arg MODEL_URL=$MODEL_URL -t $IMAGE .\ndocker push $IMAGE\n</code></pre> <p>Create a model manifest &amp; apply into a cluster with KubeAI installed. NOTE: The only difference between an built-in model image and otherwise is the addition of the <code>image:</code> field.</p> <pre><code>kubectl apply -f - &lt;&lt; EOF\napiVersion: kubeai.org/v1\nkind: Model\nmetadata:\n  name: builtin-model-example\nspec:\n  features: [\"TextGeneration\"]\n  owner: alibaba\n  image: $IMAGE # &lt;-- The image with model built-in\n  url: \"$MODEL_URL\"\n  engine: OLlama\n  resourceProfile: cpu:1\nEOF\n</code></pre>"},{"location":"how-to/cache-models-with-aws-efs/","title":"Cache models with AWS EFS","text":"<p>KubeAI can manage model caches. AWS EFS is supported as a pluggable backend store.</p> <p> </p> <p>Follow the EKS install guide.</p>"},{"location":"how-to/cache-models-with-aws-efs/#1-create-an-efs-file-system","title":"1. Create an EFS File System","text":"<p>Set environment variables to match your environment.</p> <pre><code>export CLUSTER_NAME=\"cluster-with-karpenter\"\nexport CLUSTER_REGION=\"us-west-2\"\n</code></pre> <p>Create an EFS file system in the same VPC as your EKS cluster.</p> <pre><code>vpc_id=$(aws eks describe-cluster \\\n    --name $CLUSTER_NAME \\\n    --query \"cluster.resourcesVpcConfig.vpcId\" \\\n    --output text)\n\ncidr_range=$(aws ec2 describe-vpcs \\\n    --vpc-ids $vpc_id \\\n    --query \"Vpcs[].CidrBlock\" \\\n    --output text \\\n    --region ${CLUSTER_REGION})\n\nsecurity_group_id=$(aws ec2 create-security-group \\\n    --group-name MyEfsSecurityGroup \\\n    --description \"My EFS security group\" \\\n    --vpc-id $vpc_id \\\n    --output text)\n\naws ec2 authorize-security-group-ingress \\\n    --group-id $security_group_id \\\n    --protocol tcp \\\n    --port 2049 \\\n    --cidr $cidr_range\n\nfile_system_id=$(aws efs create-file-system \\\n    --region ${CLUSTER_REGION} \\\n    --performance-mode generalPurpose \\\n    --query 'FileSystemId' \\\n    --output text)\n</code></pre> <p>Expose the EFS file system to the subnets used by your EKS cluster. <pre><code>SUBNETS=$(eksctl get cluster --region us-west-2 ${CLUSTER_NAME} -o json | jq -r '.[0].ResourcesVpcConfig.SubnetIds[]')\n\nwhile IFS= read -r subnet; do\n    echo \"Creating EFS mount target in $subnet\"\n    aws efs create-mount-target --file-system-id $file_system_id \\\n      --subnet-id $subnet --security-groups $security_group_id --output text\ndone &lt;&lt;&lt; \"$SUBNETS\"\n</code></pre></p>"},{"location":"how-to/cache-models-with-aws-efs/#2-install-the-efs-csi-driver","title":"2. Install the EFS CSI driver","text":"<pre><code>export ROLE_NAME=AmazonEKS_EFS_CSI_DriverRole\neksctl create iamserviceaccount \\\n    --name efs-csi-controller-sa \\\n    --namespace kube-system \\\n    --cluster ${CLUSTER_NAME} \\\n    --role-name ${ROLE_NAME} \\\n    --role-only \\\n    --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEFSCSIDriverPolicy \\\n    --approve\n\nTRUST_POLICY=$(aws iam get-role --role-name ${ROLE_NAME} \\\n    --query 'Role.AssumeRolePolicyDocument' --output json | \\\n    sed -e 's/efs-csi-controller-sa/efs-csi-*/' -e 's/StringEquals/StringLike/')\n\naws iam update-assume-role-policy --role-name ${ROLE_NAME} --policy-document \"$TRUST_POLICY\"\n\n# Get the role ARN\nEFS_ROLE_ARN=$(aws iam get-role --role-name AmazonEKS_EFS_CSI_DriverRole \\\n  --query 'Role.Arn' --output text)\n\naws eks create-addon --cluster-name $CLUSTER_NAME --addon-name aws-efs-csi-driver \\\n  --service-account-role-arn $EFS_ROLE_ARN\n</code></pre> <p>Wait for EKS Addon to active. <pre><code>aws eks wait addon-active --cluster-name $CLUSTER_NAME \\\n  --addon-name aws-efs-csi-driver\n</code></pre> Verify that the EFS CSI driver is running.</p> <pre><code>kubectl get daemonset efs-csi-node -n kube-system\n</code></pre> <p>Create a storage class for using EFS dynamic mode.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: efs-sc\nprovisioner: efs.csi.aws.com\nparameters:\n  provisioningMode: efs-ap\n  fileSystemId: \"${file_system_id}\"\n  directoryPerms: \"700\"\nEOF\n</code></pre> <p>Make sure to set <code>file_system_id</code> match the EFS file system ID created in the first step.</p>"},{"location":"how-to/cache-models-with-aws-efs/#3-configure-kubeai-with-the-efs-cache-profile","title":"3. Configure KubeAI with the EFS cache profile","text":"<p>You can skip this step if you've already installed KubeAI using the EKS Helm values file: values-eks.yaml.</p> <p>Configure KubeAI with the <code>efs-dynamic</code> cache profile. <pre><code>helm upgrade --install kubeai kubeai/kubeai \\\n  --reuse-values -f - &lt;&lt;EOF\ncacheProfiles:\n  efs-dynamic:\n    sharedFilesystem:\n      storageClassName: \"efs-sc\"\n  efs-static:\n    sharedFilesystem:\n      persistentVolumeName: \"efs-pv\"\nEOF\n</code></pre> You can also manually provision a PersistentVolume using EFS static mode. The efs-static profile is used for this purpose.</p>"},{"location":"how-to/cache-models-with-aws-efs/#4-configure-a-model-to-use-the-efs-cache","title":"4. Configure a model to use the EFS cache","text":"<p>Apply a Model with <code>cacheProfile</code> set to <code>efs-dynamic</code>.</p> <p>NOTE: If you already installed the models chart, you will need to edit you values file and run <code>helm upgrade</code>.</p> <pre><code>helm install kubeai-models kubeai/models -f - &lt;&lt;EOF\ncatalog:\n  llama-3.1-8b-instruct-fp8-l4:\n    enabled: true\n    cacheProfile: efs-dynamic\nEOF\n</code></pre> <p>Wait for the Model to be fully cached.</p> <pre><code>kubectl wait --timeout 10m --for=jsonpath='{.status.cache.loaded}'=true model/llama-3.1-8b-instruct-fp8-l4\n</code></pre> <p>This model will now be loaded from Filestore when it is served.</p>"},{"location":"how-to/cache-models-with-aws-efs/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/cache-models-with-aws-efs/#mountvolesetup-failed-for-volume-pvc-deadline-exceeded","title":"MountVole.SetUp failed for volume pvc deadline exceeded","text":"<p><code>kubectl get events</code> may show an error like this: <pre><code>8s          Warning   FailedMount             pod/load-cache-llama-3.1-8b-instruct-fp8-l4-w7thh      MountVolume.SetUp failed for volume \"pvc-ceedb563-1e68-47fa-9d12-c697ae153d04\" : rpc error: code = DeadlineExceeded desc = context deadline exceeded\n</code></pre></p> <p>Checking the logs of the EFS CSI DaemonSet may show an error like this: <pre><code>kubectl logs -f efs-csi-node-4n75c -n kube-system\nOutput: Could not start amazon-efs-mount-watchdog, unrecognized init system \"aws-efs-csi-dri\"\nMount attempt 1/3 failed due to timeout after 15 sec, wait 0 sec before next attempt.\nMount attempt 2/3 failed due to timeout after 15 sec, wait 0 sec before next attempt.\nb'mount.nfs4: Connection timed out'\n</code></pre></p> <p>This likely means your mount target isn't setup correctly. Possibly the security group is not allowing traffic from the EKS cluster.</p>"},{"location":"how-to/cache-models-with-aws-efs/#model-loading-job","title":"Model Loading Job","text":"<p>Check to see if there is an ongoing model loader Job.</p> <pre><code>kubectl get jobs\n</code></pre>"},{"location":"how-to/cache-models-with-gcp-filestore/","title":"Cache models with GCP Filestore","text":"<p>KubeAI can manage model caches. GCP Filestore is supported as a pluggable backend store.</p> <p> </p> <p>Follow the GKE install guide.</p> <p>Ensure that the Filestore API is enabled.</p> <pre><code>gcloud services enable file.googleapis.com\n</code></pre>"},{"location":"how-to/cache-models-with-gcp-filestore/#1-configure-kubeai-with-caching-profile","title":"1. Configure KubeAI with Caching Profile","text":"<p>You can skip this step if you've already installed KubeAI using the GKE Helm values file: values-gke.yaml.</p> <p>Configure KubeAI with the Filestore cache profiles. <pre><code>helm upgrade --install kubeai kubeai/kubeai \\\n  --reuse-values -f - &lt;&lt;EOF\ncacheProfiles:\n  standard-filestore:\n    sharedFilesystem:\n      storageClassName: \"standard-rwx\"\n  premium-filestore:\n    sharedFilesystem:\n      storageClassName: \"premium-rwx\"\nEOF\n</code></pre></p>"},{"location":"how-to/cache-models-with-gcp-filestore/#2-deploy-a-model-that-uses-the-filestore-caching-profile","title":"2. Deploy a model that uses the Filestore Caching Profile","text":"<p>Apply a Model with the cache profile set to <code>standard-filestore</code> (defined in the reference GKE Helm values file).</p> TIP: If you want to use `premium-filestore` you will need to ensure you have quota. <p>Open the cloud console quotas page: https://console.cloud.google.com/iam-admin/quotas. Make sure your project is selected in the top left.</p> <p>Ensure that you have at least 2.5Tb of <code>PremiumStorageGbPerRegion</code> quota in the region where your cluster is deployed.</p> <p></p> <p></p> <p>NOTE: If you already installed the models chart, you will need to edit you values file and run <code>helm upgrade</code>.</p> <pre><code>helm install kubeai-models kubeai/models -f - &lt;&lt;EOF\ncatalog:\n  llama-3.1-8b-instruct-fp8-l4:\n    enabled: true\n    cacheProfile: standard-filestore\nEOF\n</code></pre> <p>Wait for the Model to be fully cached. This may take a while if the Filestore instance needs to be created.</p> <pre><code>kubectl wait --timeout 10m --for=jsonpath='{.status.cache.loaded}'=true model/llama-3.1-8b-instruct-fp8-l4\n</code></pre> <p>This model will now be loaded from Filestore when it is served.</p>"},{"location":"how-to/cache-models-with-gcp-filestore/#troubleshooting","title":"Troubleshooting","text":""},{"location":"how-to/cache-models-with-gcp-filestore/#filestore-csi-driver","title":"Filestore CSI Driver","text":"<p>Ensure that the Filestore CSI driver is enabled by checking for the existance of Kubernetes storage classes. If they are not found, follow the GCP guide for enabling the CSI driver.</p> <pre><code>kubectl get storageclass standard-rwx premium-rwx\n</code></pre>"},{"location":"how-to/cache-models-with-gcp-filestore/#persistentvolumes","title":"PersistentVolumes","text":"<p>Check the PersistentVolumeClaim (that should be created by KubeAI).</p> <pre><code>kubectl describe pvc shared-model-cache-\n</code></pre> Example: Out-of-quota error <pre><code>  Warning  ProvisioningFailed    11m (x26 over 21m)  filestore.csi.storage.gke.io_gke-50826743a27a4d52bf5b-7fac-9607-vm_b4bdb2ec-b58b-4363-adec-15c270a14066  failed to provision volume with StorageClass \"premium-rwx\": rpc error: code = ResourceExhausted desc = googleapi: Error 429: Quota limit 'PremiumStorageGbPerRegion' has been exceeded. Limit: 0 in region us-central1.\nDetails:\n[\n  {\n    \"@type\": \"type.googleapis.com/google.rpc.QuotaFailure\",\n    \"violations\": [\n      {\n        \"description\": \"Quota 'PremiumStorageGbPerRegion' exhausted. Limit 0 in region us-central1\",\n        \"subject\": \"project:819220466562\"\n      }\n    ]\n  }\n]\n</code></pre> <p>Check to see if the PersistentVolume has been fully provisioned.</p> <pre><code>kubectl get pv\n# Find name of corresponding pv...\nkubectl describe pv &lt;name&gt;\n</code></pre>"},{"location":"how-to/cache-models-with-gcp-filestore/#model-loading-job","title":"Model Loading Job","text":"<p>Check to see if there is an ongoing model loader Job.</p> <pre><code>kubectl get jobs\n</code></pre>"},{"location":"how-to/configure-autoscaling/","title":"Configure autoscaling","text":"<p>This guide will cover how to configure KubeAI autoscaling parameters.</p>"},{"location":"how-to/configure-autoscaling/#system-settings","title":"System Settings","text":"<p>KubeAI administrators can define system-wide autoscaling settings by setting the following Helm values (for the <code>kubeai/kubeai</code> chart):</p> <p>Example:</p> <pre><code># helm-values.yaml\nmodelAutoscaling:\n  interval: 15s\n  timeWindow: 10m\n# ...\n</code></pre>"},{"location":"how-to/configure-autoscaling/#model-settings","title":"Model Settings","text":"<p>The following settings can be configured on a model-by-model basis.</p>"},{"location":"how-to/configure-autoscaling/#model-settings-helm","title":"Model settings: helm","text":"<p>If you are managing models via the <code>kubeai/models</code> Helm chart, you can use:</p> <pre><code># helm-values.yaml\ncatalog:\n  model-a:\n    # ...\n    minReplicas: 1\n    maxReplicas: 9\n    targetRequests: 250\n    scaleDownDelaySeconds: 45\n  model-b:\n    # ...\n    disableAutoscaling: true\n# ...\n</code></pre> <p>Re-running <code>helm upgrade</code> with these additional parameters will update model settings in the cluster.</p>"},{"location":"how-to/configure-autoscaling/#model-settings-kubectl","title":"Model settings: kubectl","text":"<p>You can also specify the autoscaling profile directly via the Models custom resource in the Kubernetes API:</p> <pre><code>apiVersion: kubeai.org/v1\nkind: Model\nmetadata:\n  name: my-model\nspec:\n  # ...\n  minReplicas: 1\n  maxReplicas: 9\n  targetRequests: 250\n  scaleDownDelaySeconds: 45\n</code></pre> <p>If you are already managing models using Model manifest files, you can make the update to your file and reapply it using <code>kubectl apply -f &lt;filename&gt;.yaml</code>.</p>"},{"location":"how-to/configure-embedding-models/","title":"Configure embedding models","text":"<p>KubeAI supports the following engines for text embedding models:</p> <ul> <li>Infinity</li> <li>vLLM</li> <li>Ollama</li> </ul> <p>Infinity supports any HuggingFace models listed as text-embedding. See the models, reranking or clip models on huggingface for reference.</p>"},{"location":"how-to/configure-embedding-models/#install-baaibge-small-en-v15-model-using-infinity","title":"Install BAAI/bge-small-en-v1.5 model using Infinity","text":"<p>Create a file named <code>kubeai-models.yaml</code> with the following content:</p> <pre><code>catalog:\n  bge-embed-text-cpu:\n    enabled: true\n    features: [\"TextEmbedding\"]\n    owner: baai\n    url: \"hf://BAAI/bge-small-en-v1.5\"\n    engine: Infinity\n    resourceProfile: cpu:1\n    minReplicas: 1\n</code></pre> <p>Apply the kubeai-models helm chart:</p> <pre><code>helm install kubeai-models kubeai/models -f ./kubeai-models.yaml\n</code></pre> <p>Once the pod is ready, you can use the OpenAI Python SDK to interact with the model:</p> <pre><code>from openai import OpenAI\n# Assumes port-forward of kubeai service to localhost:8000.\nclient = OpenAI(api_key=\"ignored\", base_url=\"http://localhost:8000/openai/v1\")\nresponse = client.embeddings.create(\n    input=\"Your text goes here.\",\n    model=\"bge-embed-text-cpu\"\n)\n</code></pre>"},{"location":"how-to/configure-reranking-models/","title":"Configure reranking models","text":"<p>KubeAI supports reranking models via the Infinity and vLLM (Recommended for GPU) engine.</p>"},{"location":"how-to/configure-reranking-models/#install-baaibge-reranker-base-model-using-infinity","title":"Install BAAI/bge-reranker-base model using Infinity","text":"<p>Create a file named <code>kubeai-rerank-models.yaml</code> with the following content:</p> <pre><code>catalog:\n  bge-rerank-base-cpu:\n    enabled: true\n    features: [\"Reranking\"]\n    owner: baai\n    url: \"hf://BAAI/bge-reranker-base\"\n    engine: Infinity\n    #engine: VLLM\n    resourceProfile: cpu:1\n    # resourceProfile: nvidia-gpu-l4:1\n    minReplicas: 1\n</code></pre> <p>Apply the kubeai-models helm chart:</p> <pre><code>helm install kubeai-models kubeai/models -f ./kubeai-rerank-models.yaml.yaml\n</code></pre> <p>Once the pod is ready, you can call the rerank endpoint:</p> <pre><code>import requests\nresp = requests.post(\n    \"http://localhost:8000/vllm/v1/rerank\",\n    json={\n        \"model\": \"bge-rerank-base-cpu\",\n        \"query\": \"Which document talks about apples?\",\n        \"documents\": [\"An apple a day keeps the doctor away\", \"Oranges are tasty\"],\n    },\n)\nprint(resp.json())\n</code></pre>"},{"location":"how-to/configure-resource-profiles/","title":"Configure resource profiles","text":"<p>This guide will cover modifying preconfigured resource profiles and adding your own.</p>"},{"location":"how-to/configure-resource-profiles/#modifying-preconfigured-resource-profiles","title":"Modifying preconfigured resource profiles","text":"<p>The KubeAI helm chart comes with preconfigured resource profiles for common resource types such as NVIDIA L4 GPUs. You can view these profiles in the default helm values file.</p> <p>These profiles usually require some additional settings based on the cluster/cloud that KubeAI is installed into. You can modify a resource profile by setting custom helm values and runing <code>helm install</code> or <code>helm upgrade</code>. For example, if you are installing KubeAI on GKE you will need to set GKE-specific node selectors:</p> <pre><code># helm-values.yaml\nresourceProfiles:\n  nvidia-gpu-l4:\n    nodeSelector:\n      cloud.google.com/gke-accelerator: \"nvidia-l4\"\n      cloud.google.com/gke-spot: \"true\"\n</code></pre> <p>NOTE: See the cloud-specific installation guide for a comprehensive list of settings.</p>"},{"location":"how-to/configure-resource-profiles/#adding-additional-resource-profiles","title":"Adding additional resource profiles","text":"<p>If the preconfigured resource profiles do not meet your needs you can add additional profiles by appending to the <code>.resourceProfiles</code> object in the helm values file you use to install KubeAI.</p> <pre><code># helm-values.yaml\nresourceProfiles:\n  my-custom-gpu:\n    imageName: \"optional-custom-image-name\"\n    nodeSelector:\n      my-custom-node-pool: \"some-value\"\n    limits:\n      custom.com/gpu: \"1\"\n    requests:\n      custom.com/gpu: \"1\"\n      cpu: \"3\"\n      memory: \"12Gi\"\n    schedulerName: \"my-custom-scheduler\"\n    runtimeClassName: \"my-custom-runtime-class\"\n</code></pre> <p>If you need to run custom model server images on your resource profile, make sure to also add those in the <code>modelServers</code> section:</p> <pre><code># helm-values.yaml\nmodelServers:\n  VLLM:\n    images:\n      optional-custom-image-name: \"my-repo/my-vllm-image:v1.2.3\"\n  OLlama:\n    images:\n      optional-custom-image-name: \"my-repo/my-ollama-image:v1.2.3\"\n</code></pre>"},{"location":"how-to/configure-resource-profiles/#next","title":"Next","text":"<p>See the guide on how to install models which includes how to configure the resource profile to use for a given model.</p>"},{"location":"how-to/configure-speech-to-text/","title":"Configure speech-to-text","text":"<p>KubeAI provides a Speech to Text endpoint that can be used to transcribe audio files. This guide will walk you through the steps to enable this feature.</p>"},{"location":"how-to/configure-speech-to-text/#enable-speech-to-text-model","title":"Enable Speech to Text model","text":"<p>You can create new models by creating a Model CRD object or by enabling a model from the model catalog.</p>"},{"location":"how-to/configure-speech-to-text/#enable-from-model-catalog","title":"Enable from model catalog","text":"<p>KubeAI provides predefined models in the <code>kubeai/models</code> Helm chart. To enable the Speech to Text model, you can set the <code>enabled</code> flag to <code>true</code> in your values file.</p> <pre><code># models-helm-values.yaml\ncatalog:\n  faster-whisper-medium-en-cpu:\n    enabled: true\n    minReplicas: 1\n</code></pre>"},{"location":"how-to/configure-speech-to-text/#enable-by-creating-model","title":"Enable by creating Model","text":"<p>You can also create a Model object to enable the Speech to Text model. For example:</p> <pre><code>apiVersion: kubeai.org/v1\nkind: Model\nmetadata:\n  name: faster-whisper-medium-en-cpu\nspec:\n  features: [SpeechToText]\n  owner: Systran\n  url: hf://Systran/faster-whisper-medium.en\n  engine: FasterWhisper\n  resourceProfile: cpu:1\n</code></pre>"},{"location":"how-to/configure-speech-to-text/#usage","title":"Usage","text":"<p>The Speech to Text endpoint is available at <code>/openai/v1/transcriptions</code>.</p> <p>Example usage using curl:</p> <pre><code>curl -L -o kubeai.mp4 https://github.com/user-attachments/assets/711d1279-6af9-4c6c-a052-e59e7730b757\ncurl http://localhost:8000/openai/v1/audio/transcriptions \\\n  -F \"file=@kubeai.mp4\" \\\n  -F \"language=en\" \\\n  -F \"model=faster-whisper-medium-en-cpu\"\n</code></pre>"},{"location":"how-to/configure-text-generation-models/","title":"Configure Text Generation Models","text":"<p>KubeAI supports the following engines for text generation models (LLMs, VLMs, ..):</p> <ul> <li>vLLM (Recommended for GPU)</li> <li>Ollama (Recommended for CPU)</li> <li>Need something else? Please file an issue on GitHub.</li> </ul> <p>There are 2 ways to install a text generation model in KubeAI: - Use Helm with the <code>kubeai/models</code> chart. - Use <code>kubectl apply -f model.yaml</code> to install a Model Custom Resource.</p> <p>KubeAI comes with pre-validated and optimized Model configurations for popular text generation models. These models are available in the kubeai/models Helm chart and are also published as raw manifests in the manifests/model directory.</p> <p>You can also easily define your own models using the Model Custom Resource directly or by using the <code>kubeai/models</code> Helm chart.</p>"},{"location":"how-to/configure-text-generation-models/#install-a-text-generation-model-using-helm","title":"Install a Text Generation Model using Helm","text":"<p>You can take a look at all the pre-configured models in the chart's default values file.</p> <p>You can get the default values for the models chart using the following command: <pre><code>helm show values kubeai/models\n</code></pre></p>"},{"location":"how-to/configure-text-generation-models/#install-text-generation-model-using-l4-gpu","title":"Install Text Generation Model using L4 GPU","text":"<p>Enable the Llama 3.1 8B model using the Helm chart:</p> <pre><code>helm upgrade --install --reuse-values kubeai-models kubeai/models -f - &lt;&lt;EOF\ncatalog:\n  llama-3.1-8b-instruct-fp8-l4:\n    enabled: true\n    engine: VLLM\n    resourceProfile: nvidia-gpu-l4:1\n    minReplicas: 1 # by default this is 0\nEOF\n</code></pre>"},{"location":"how-to/configure-text-generation-models/#install-a-text-generation-model-using-kubectl","title":"Install a Text Generation Model using kubectl","text":"<p>You can use the Model Custom Resource directly to install a model using <code>kubectl apply -f model.yaml</code>.</p>"},{"location":"how-to/configure-text-generation-models/#install-text-generation-model-using-l4-gpu_1","title":"Install Text Generation Model using L4 GPU","text":"<p>Apply the following Model Custom Resource to install the Llama 3.1 8B model using vLLM on L4 GPU: <pre><code>apiVersion: kubeai.org/v1\nkind: Model\nmetadata:\n  name: llama-3.1-8b-instruct-fp8-l4\nspec:\n  features: [TextGeneration]\n  url: hf://neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8\n  engine: VLLM\n  args:\n    - --max-model-len=16384\n    - --max-num-batched-token=16384\n    - --gpu-memory-utilization=0.9\n    - --disable-log-requests\n  resourceProfile: nvidia-gpu-l4:1\n</code></pre></p>"},{"location":"how-to/configure-text-generation-models/#configure-a-chat-template","title":"Configure a Chat Template","text":"<p>Some models do not ship will chat templates and some engines such as vLLM do not provide a default one. In these cases, you can use <code>.spec.files</code> to inject a template at Pod runtime.</p> <pre><code>kind: Model\nspec:\n  # ...\n  engine: VLLM\n  args:\n  - --chat-template=/config/chat-template.jinja\n  files:\n  - path: /config/chat-template.jinja\n    content: |\n      {% for message in messages %}{{'&lt;|im_start|&gt;' + message['role'] + '\\n' + message['content']}}{% if (loop.last and add_generation_prompt) or not loop.last %}{{ '&lt;|im_end|&gt;' + '\\n'}}{% endif %}{% endfor %}\n      {% if add_generation_prompt and messages[-1]['role'] != 'assistant' %}{{ '&lt;|im_start|&gt;assistant\\n' }}{% endif %}\n</code></pre>"},{"location":"how-to/configure-text-generation-models/#interact-with-the-text-generation-model","title":"Interact with the Text Generation Model","text":"<p>The KubeAI service exposes an OpenAI compatible API that you can use to query the available models and interact with them.</p> <p>The KubeAI service is available at <code>http://kubeai/openai/v1</code> within the Kubernetes cluster.</p> <p>You can also port-forward the KubeAI service to your local machine to interact with the models:</p> <pre><code>kubectl port-forward svc/kubeai 8000:80\n</code></pre> <p>You can now query the available models using curl:</p> <pre><code>curl http://localhost:8000/openai/v1/models\n</code></pre>"},{"location":"how-to/configure-text-generation-models/#using-curl-to-interact-with-the-model","title":"Using curl to interact with the model","text":"<p>Run the following curl command to interact with the model named <code>llama-3.1-8b-instruct-fp8-l4</code>: <pre><code>curl \"http://localhost:8000/openai/v1/chat/completions\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"llama-3.1-8b-instruct-fp8-l4\",\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"Write a haiku about recursion in programming.\"\n            }\n        ]\n    }'\n</code></pre></p>"},{"location":"how-to/configure-text-generation-models/#using-the-openai-python-sdk-to-interact-with-the-model","title":"Using the OpenAI Python SDK to interact with the model","text":"<p>Once the pod is ready, you can use the OpenAI Python SDK to interact with the model: All OpenAI SDKs work with KubeAI since the KubeAI service is OpenAI API compatible.</p> <p>See the below example code to interact with the model using the OpenAI Python SDK: <pre><code>import os\nfrom openai import OpenAI\n# Assumes port-forward of kubeai service to localhost:8000.\nkubeai_endpoint = \"http://localhost:8000/openai/v1\"\nmodel_name = \"llama-3.1-8b-instruct-fp8-l4\"\n\n# If you are running in a Kubernetes cluster, you can use the kubeai service endpoint.\nif os.getenv(\"KUBERNETES_SERVICE_HOST\"):\n    kubeai_endpoint = \"http://kubeai/openai/v1\"\n\nclient = OpenAI(api_key=\"ignored\", base_url=kubeai_endpoint)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Say this is a test\",\n        }\n    ],\n    model=model_name,\n)\n</code></pre></p>"},{"location":"how-to/configure-text-generation-models/#ollama-configuration-notes","title":"Ollama Configuration Notes","text":""},{"location":"how-to/configure-text-generation-models/#insecure-model-pulling","title":"Insecure Model Pulling","text":"<p>To allow pulling Ollama models from insecure registries (e.g., HTTP-only or self-signed TLS), append the <code>?insecure=true</code> query parameter to the model URL.</p> <p>Warning: Use this only in trusted network environments.</p> <p>Example Model spec using an insecure registry: <pre><code>spec:\n  url: ollama://my-local-registry:5000/my-model?insecure=true\n</code></pre></p>"},{"location":"how-to/configure-text-generation-models/#disable-model-pulling","title":"Disable Model Pulling","text":"<p>To disable model pulling from Ollama, set the <code>pull</code> field to <code>false</code> in the Model spec. This is useful when you want to use a model that is already available in the Ollama server.</p> <p>Example Model spec to disable pulling: <pre><code>spec:\n  url: ollama://my-local-registry:5000/my-model?pull=false\n</code></pre></p>"},{"location":"how-to/install-models/","title":"Install models","text":"<p>This guide provides instructions on how to configure KubeAI models.</p>"},{"location":"how-to/install-models/#installing-models-with-helm","title":"Installing models with helm","text":"<p>KubeAI provides a chart that contains preconfigured models.</p>"},{"location":"how-to/install-models/#preconfigured-models-with-helm","title":"Preconfigured models with helm","text":"<p>When you are defining Helm values for the <code>kubeai/models</code> chart you can install a preconfigured Model by setting <code>enabled: true</code>. You can view a list of all preconfigured models in the chart's default values file. </p> <pre><code># helm-values.yaml\ncatalog:\n  llama-3.1-8b-instruct-fp8-l4:\n    enabled: true\n</code></pre> <p>You can optionally override preconfigured settings, for example, <code>resourceProfile</code>:</p> <pre><code># helm-values.yaml\ncatalog:\n  llama-3.1-8b-instruct-fp8-l4:\n    enabled: true\n    resourceProfile: nvidia-gpu-l4:2 # Require \"2 NVIDIA L4 GPUs\"\n</code></pre>"},{"location":"how-to/install-models/#custom-models-with-helm","title":"Custom models with helm","text":"<p>If you prefer to add a custom model via the same Helm chart you use for installed KubeAI, you can add your custom model entry into the <code>.catalog</code> array of your existing values file for the <code>kubeai/models</code> Helm chart:</p> <pre><code># helm-values.yaml\ncatalog:\n  my-custom-model-name:\n    enabled: true\n    features: [\"TextEmbedding\"]\n    owner: me\n    url: \"hf://me/my-custom-model\"\n    resourceProfile: CPU:1\n</code></pre>"},{"location":"how-to/install-models/#installing-models-with-kubectl","title":"Installing models with kubectl","text":"<p>You can add your own model by defining a Model yaml file and applying it using <code>kubectl apply -f model.yaml</code>.</p> <p>Take a look at the KubeAI API docs to view Model schema documentation.</p> <p>If you have a running cluster with KubeAI installed you can inspect the schema for a Model using <code>kubectl explain</code>:</p> <pre><code>kubectl explain models\nkubectl explain models.spec\nkubectl explain models.spec.engine\n</code></pre> <p>You can view all example manifests on the GitHub repository.</p> <p>Below are few examples using various engines and resource profiles.</p>"},{"location":"how-to/install-models/#example-gemma-2-2b-using-ollama-on-cpu","title":"Example Gemma 2 2B using Ollama on CPU","text":"<pre><code>apiVersion: kubeai.org/v1\nkind: Model\nmetadata:\n  name: gemma2-2b-cpu\nspec:\n  features: [TextGeneration]\n  url: ollama://gemma2:2b\n  engine: OLlama\n  resourceProfile: cpu:2\n</code></pre>"},{"location":"how-to/install-models/#example-llama-31-8b-using-vllm-on-nvidia-l4-gpu","title":"Example Llama 3.1 8B using vLLM on NVIDIA L4 GPU","text":"<pre><code>apiVersion: kubeai.org/v1\nkind: Model\nmetadata:\n  name: llama-3.1-8b-instruct-fp8-l4\nspec:\n  features: [TextGeneration]\n  owner: neuralmagic\n  url: hf://neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8\n  engine: VLLM\n  args:\n    - --max-model-len=16384\n    - --max-num-batched-token=16384\n    - --gpu-memory-utilization=0.9\n    - --disable-log-requests\n  resourceProfile: nvidia-gpu-l4:1\n</code></pre>"},{"location":"how-to/install-models/#programmatically-installing-models","title":"Programmatically installing models","text":"<p>See the examples.</p>"},{"location":"how-to/install-models/#calling-a-model","title":"Calling a model","text":"<p>You can inference a model by calling the KubeAI OpenAI compatible API. The model name should match the KubeAI model name.</p>"},{"location":"how-to/install-models/#using-pod-priority-classes-for-model-preemption","title":"Using Pod Priority Classes for Model Preemption","text":"<p>You can use Kubernetes Pod Priority and Preemption to configure your models with different priority levels. This is useful when you have limited resources and want to ensure that high-priority models can preempt lower-priority models when necessary.</p> <p>First, create the necessary PriorityClasses in your cluster:</p> <pre><code>apiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: high-priority\nvalue: 1000000  # Higher value means higher priority\nglobalDefault: false\ndescription: \"This priority class should be used for critical inference models only.\"\n---\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: medium-priority\nvalue: 100000\nglobalDefault: false\ndescription: \"This priority class should be used for medium priority inference models.\"\n---\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: low-priority\nvalue: 10000\nglobalDefault: false\ndescription: \"This priority class should be used for low priority inference models.\"\n</code></pre> <p>Then, assign these priority classes to your models:</p> <pre><code>apiVersion: kubeai.org/v1\nkind: Model\nmetadata:\n  name: critical-service-model\nspec:\n  features: [TextGeneration]\n  url: hf://neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8\n  engine: VLLM\n  resourceProfile: nvidia-gpu-l4:1\n  priorityClassName: high-priority\n---\napiVersion: kubeai.org/v1\nkind: Model\nmetadata:\n  name: background-research-model\nspec:\n  features: [TextGeneration]\n  url: ollama://gemma2:2b\n  engine: OLlama\n  resourceProfile: cpu:2\n  priorityClassName: low-priority\n</code></pre> <p>When the cluster is under resource pressure, Kubernetes will evict lower-priority model pods to make room for higher-priority model pods. This ensures that your most critical models remain available even during resource constraints.</p>"},{"location":"how-to/install-models/#feedback-welcome-a-model-management-ui","title":"Feedback welcome: A model management UI","text":"<p>We are considering adding a UI for managing models in a running KubeAI instance. Give the GitHub Issue a thumbs up if you would be interested in this feature.</p>"},{"location":"how-to/load-models-from-pvc/","title":"Load Models from PVC","text":"<p>You can store your models in a Persistent Volume Claim (PVC) and let KubeAI use them for serving. Both vLLM and Ollama engines support loading models from PVCs.</p> <p>You must ensure the model files are already present in the PVC before creating the Model resource. Alternatively you can use KubeAI's native caching mechanism which downloads the model for you:</p> <ul> <li>Cache Models with GCP Filestore</li> <li>Cache Models with EFS</li> </ul>"},{"location":"how-to/load-models-from-pvc/#vllm","title":"vLLM","text":"<p>For vLLM, use the following URL format: <pre><code>url: pvc://$PVC_NAME          # Loads the model from the PVC named $PVC_NAME\nurl: pvc://$PVC_NAME/$PATH    # Loads from a specific path within the PVC\n</code></pre></p>"},{"location":"how-to/load-models-from-pvc/#pvc-requirements","title":"PVC requirements","text":"<p>vLLM supports both ReadWriteMany and ReadOnlyMany access modes. <code>Many</code> is used in order to support more than 1 vLLM replica.</p>"},{"location":"how-to/load-models-from-pvc/#ollama","title":"Ollama","text":"<p>For Ollama, use the following URL formats: <pre><code>url: pvc://$PVC_NAME?model=$MODEL_NAME    # Loads the model named $MODEL_NAME that's loaded on the disk\nurl: pvc://$PVC_NAME/$PATH?model=$MODEL_NAME\n</code></pre></p>"},{"location":"how-to/load-models-from-pvc/#pvc-requirements_1","title":"PVC Requirements","text":"<p>Ollama requires using ReadWriteMany access mode because the rename operation <code>ollama cp</code> needs to write to the PVC.</p>"},{"location":"how-to/load-models-from-pvc/#example-loading-qwen-05b-from-pvc","title":"Example: Loading Qwen 0.5b from PVC","text":"<ol> <li>Create a PVC with ReadWriteMany named <code>model-pvc</code>. See example.</li> <li> <p>Create a K8s Job to load the model onto `model-pvc. See example</p> <p>The PVC should now have a <code>blobs/</code> and <code>manifests/</code> directory after the loader completes.</p> </li> <li> <p>Create a Model to load from PVC:</p> </li> </ol> <pre><code>url: pvc://model-pvc?model=qwen:0.5b\n</code></pre>"},{"location":"how-to/observability-with-prometheus-stack/","title":"Configure Observability with Prometheus Stack","text":"<p>KubeAI provides a vLLM PodMonitor resource to scrape vLLM metrics.</p> <p>KubeAI also provides a Grafana dashboard.</p>"},{"location":"how-to/observability-with-prometheus-stack/#deploying-prometheus-operator","title":"Deploying Prometheus Operator","text":"<p>The Prometheus Operator is a Kubernetes operator that manages Prometheus and its related components. The Prometheus Stack includes Grafana and Prometheus.</p> <p>Add Prometheus Helm repo: <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\n</code></pre></p> <p>Install the Prometheus Stack and ensure PodMonitor works without special labels: <pre><code>helm install prometheus prometheus-community/kube-prometheus-stack -f - &lt;&lt;EOF\nprometheus:\n  prometheusSpec:\n    podMonitorSelectorNilUsesHelmValues: false\n    ruleSelectorNilUsesHelmValues: false\n    serviceMonitorSelectorNilUsesHelmValues: false\n    probeSelectorNilUsesHelmValues: false\nEOF\n</code></pre></p>"},{"location":"how-to/observability-with-prometheus-stack/#enable-kubeai-vllm-podmonitor","title":"Enable KubeAI vLLM PodMonitor","text":"<p>The podMonitor can be enabled by using the following KubeAI helm values:</p> <pre><code>metrics:\n  prometheusOperator:\n    vLLMPodMonitor:\n      # Enable creation of PodMonitor resource that scrapes vLLM metrics endpoint.\n      enabled: true\n      labels: {}\n</code></pre> <p>If you want to manually create the PodMonitor please take a look at the KubeAI helm chart vLLM PodMonitor template.</p> <p>Install the KubeAI provided PodMonitor using Helm:</p> <pre><code>helm upgrade --reuse-values --install kubeai kubeai/kubeai \\\n  --set metrics.prometheusOperator.vLLMPodMonitor.enabled=true\n</code></pre>"},{"location":"how-to/observability-with-prometheus-stack/#importing-the-vllm-grafana-dashboard","title":"Importing the vLLM Grafana Dashboard","text":"<p>Now you can configure a port forward to the Grafana service: <pre><code>kubectl port-forward svc/prometheus-grafana 8081:80\n</code></pre></p> <p>Open your browser at http://localhost:8081 and log in with the default credentials (admin/prom-operator).</p> <p>You can get the credential if that doesn't work: <pre><code>kubectl get secret prometheus-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n</code></pre></p> <p>You can import the example vLLM dashboard in the KubeAI repo at examples/observability/vllm-grafana-dashboard.json.</p>"},{"location":"how-to/serve-lora-adapters/","title":"Serve LoRA adapters","text":"<p>In this guide you will configure KubeAI to serve LoRA adapters.</p>"},{"location":"how-to/serve-lora-adapters/#configuring-adapters","title":"Configuring adapters","text":"<p>LoRA adapters are configured on Model objects. For Example:</p> <pre><code># model.yaml\napiVersion: kubeai.org/v1\nkind: Model\nmetadata:\n  name: tinyllama-chat\nspec:\n  features: [TextGeneration]\n  owner: meta-llama\n  url: hf://TinyLlama/TinyLlama-1.1B-Chat-v0.3\n  adapters: # &lt;--\n  - name: colorist\n    url: hf://jashing/tinyllama-colorist-lora\n  engine: VLLM\n  resourceProfile: nvidia-gpu-l4:1\n  minReplicas: 1\n</code></pre> <p>Limitation: Currently LoRA adapters are only supported with <code>engine: VLLM</code> and <code>hf://</code> or <code>s3://</code> urls.</p> <p>You can install this Model using kubectl:</p> <pre><code>kubectl apply -f ./model.yaml\n</code></pre> <p>Or if you are managed models with the KubeAI models helm chart you can add adapters to a given model via your helm values:</p> <pre><code># helm-values.yaml\ncatalog:\n  llama-3.1-8b-instruct-fp8-l4:\n    enabled: true\n    adapters:\n    - name: example\n      url: hf://some-huggingface-user/some-huggingface-repo\n    # ...\n</code></pre>"},{"location":"how-to/serve-lora-adapters/#requesting-an-adapter","title":"Requesting an adapter","text":"<p>When using the OpenAI compatible REST API, model adapters are referenced using the <code>&lt;base-model&gt;_&lt;adapter&gt;</code> convention. Once a Model is installed with an adapter, you can request that adapter by name via appending <code>_&lt;adapter-name&gt;</code> to the model field. This will work with any OpenAI client library.</p> <p>If you installed a Model with <code>name: llama-3.2</code> and configured <code>.spec.adapters[]</code> to contain an adapter with <code>name: sql</code>, you can issue a completion request to that adapter using:</p> <pre><code>curl http://$KUBEAI_ENDPOINT/openai/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"prompt\": \"Hi\", \"model\": \"llama-3.2_sql\"}'\n</code></pre>"},{"location":"how-to/serve-lora-adapters/#listing-adapters","title":"Listing adapters","text":"<p>Adapters will be returned by the <code>/models</code> endpoint:</p> <pre><code>curl http://$KUBEAI_ENDPOINT/openai/v1/models\n</code></pre> <p>Each adapter will be listed as a separate model object with the adapter name appended to the base Model name.</p>"},{"location":"installation/aks/","title":"Install on AKS","text":"TIP: Make sure you have the right quota and permissions. <ul> <li>Confirm you have enough quota for GPU-enabled node SKUs (like <code>Standard_NC48ads_A100_v4</code>).</li> <li>Verify that you have the right permissions and that the account you use with Azure CLI can create AKS clusters, node pools, etc.</li> </ul>"},{"location":"installation/aks/#installing-prerequisites","title":"Installing Prerequisites","text":"<p>Before running this setup, ensure you have:</p> <ul> <li>Azure CLI</li> <li>Kubectl</li> <li>Helm</li> </ul>"},{"location":"installation/aks/#1-define-environment-variables","title":"1. Define environment variables","text":"<pre><code>AZURE_RESOURCE_GROUP=\"${AZURE_RESOURCE_GROUP:-kubeai-stack}\"\nAZURE_REGION=\"${AZURE_REGION:-southcentralus}\"\nCLUSTER_NAME=\"${CLUSTER_NAME:-kubeai-stack}\"\nUSER_NAME=\"${USER_NAME:-azureuser}\"\nGPU_NODE_POOL_NAME=\"${GPU_NODE_POOL_NAME:-gpunodes}\"\nGPU_NODE_COUNT=\"${GPU_NODE_COUNT:-1}\"\nGPU_VM_SIZE=\"${GPU_VM_SIZE:-Standard_NC48ads_A100_v4}\"\n</code></pre> <ul> <li><code>AZURE_RESOURCE_GROUP</code>: The name of the Azure resource group. The default value is <code>kubeai-stack</code>.</li> <li><code>AZURE_REGION</code>: The Azure location or region where the AKS cluster will be deployed. The default value is <code>southcentralus</code>.</li> <li><code>CLUSTER_NAME</code>: The name of the AKS cluster. The default value is <code>kubeai-stack</code>.</li> <li><code>USER_NAME</code>: The username for the AKS cluster. The default value is <code>azureuser</code>.</li> <li><code>GPU_NODE_POOL_NAME</code>: The name of the GPU node pool. The default value is <code>gpunodes</code>.</li> <li><code>GPU_NODE_COUNT</code>: The number of GPU nodes in the GPU node pool. The default value is <code>1</code>.</li> <li><code>GPU_VM_SIZE</code>: The SKU of the GPU VMs in the GPU node pool. The default value is <code>Standard_NC48ads_A100_v4</code>. You can find more GPU VM sizes here.</li> </ul>"},{"location":"installation/aks/#2-create-a-resource-group","title":"2. Create a Resource Group","text":"<pre><code>az group create \\\n  --name \"${AZURE_RESOURCE_GROUP}\" \\\n  --location \"${AZURE_REGION}\"\n</code></pre>"},{"location":"installation/aks/#3-create-an-aks-cluster-cpu-only","title":"3. Create an AKS Cluster (CPU Only)","text":"<p>If you only intend to run CPU-based models, you can create a simple AKS cluster as follows:</p> <pre><code>az aks create \\\n    --resource-group \"${AZURE_RESOURCE_GROUP}\" \\\n    --name \"${CLUSTER_NAME}\" \\\n    --enable-oidc-issuer \\\n    --enable-workload-identity \\\n    --enable-managed-identity \\\n    --node-count 1 \\\n    --location \"${AZURE_REGION}\" \\\n    --admin-username \"${USER_NAME}\" \\\n    --generate-ssh-keys \\\n    --os-sku Ubuntu\n</code></pre>"},{"location":"installation/aks/#4-optional-add-a-gpu-node-pool","title":"4. (Optional) Add a GPU Node Pool","text":"<p>If you want to run GPU-backed models, you need a GPU node pool. Below is an example using the <code>Standard_NC48ads_A100_v4</code> SKU. Feel free to adjust <code>--node-vm-size</code> to match a GPU SKU you have quota for:</p> <pre><code>az aks nodepool add \\\n  --resource-group \"${AZURE_RESOURCE_GROUP}\" \\\n  --cluster-name \"${CLUSTER_NAME}\" \\\n  --name \"${GPU_NODE_POOL_NAME}\" \\\n  --node-count \"${GPU_NODE_COUNT}\" \\\n  --node-vm-size \"${GPU_VM_SIZE}\" \\\n  --enable-cluster-autoscaler \\\n  --min-count 1 \\\n  --max-count 3\n</code></pre>"},{"location":"installation/aks/#5-get-aks-credentials","title":"5. Get AKS Credentials","text":"<p>Download and merge the kubeconfig for your AKS cluster:</p> <pre><code>az aks get-credentials \\\n  --resource-group \"${AZURE_RESOURCE_GROUP}\" \\\n  --name \"${CLUSTER_NAME}\" \\\n  --overwrite-existing\n</code></pre>"},{"location":"installation/aks/#6-install-the-nvidia-device-plugin-for-gpu-clusters","title":"6. Install the NVIDIA Device Plugin (for GPU Clusters)","text":"<p>If you created a GPU node pool, install the NVIDIA device plugin so Pods can request GPUs:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.17.1/deployments/static/nvidia-device-plugin.yml\n</code></pre> <p>Confirm the DaemonSet is running:</p> <pre><code>kubectl get daemonset nvidia-device-plugin-daemonset -n kube-system\n</code></pre>"},{"location":"installation/aks/#7-install-kubeai","title":"7. Install KubeAI","text":"<p>Add the KubeAI Helm repository and update:</p> <pre><code>helm repo add kubeai https://www.kubeai.org\nhelm repo update\n</code></pre> <p>(Optional) If you plan to use private or TOS-protected models from Hugging Face, export your token:</p> <pre><code>export HUGGING_FACE_HUB_TOKEN=&lt;YOUR_HF_TOKEN&gt;\n</code></pre> <p>Install KubeAI:</p> <pre><code>helm upgrade --install kubeai kubeai/kubeai \\\n  --set secrets.huggingface.token=\"${HUGGING_FACE_HUB_TOKEN}\" \\\n  --wait\n</code></pre>"},{"location":"installation/aks/#resource-profiles","title":"Resource Profiles","text":"<p>For AKS GPU deployments using the upstream Kubernetes device plugin, you can customize resource requests and limits via KubeAI\u2019s values-nvidia-k8s-device-plugin.yaml.</p> <p>We recommend referencing the installation guide for examples of how to specify these resource profiles within your Helm values. This file contains preconfigured resource profiles you can adjust for your environment.</p>"},{"location":"installation/aks/#8-deploying-models","title":"8. Deploying models","text":"<p>Take a look at the following how-to guides to deploy models:</p> <ul> <li>Configure Text Generation Models</li> <li>Configure Embedding Models</li> <li>Configure Reranking Models</li> <li>Configure Speech to Text Models</li> </ul>"},{"location":"installation/any/","title":"Install on any Kubernetes Cluster","text":"<p>KubeAI can be installed on any Kubernetes cluster and doesn't require GPUs. If you do have GPUs, then KubeAI can take advantage of them.</p> <p>Please follow the Installation using GPUs section if you have GPUs available.</p>"},{"location":"installation/any/#prerequisites","title":"Prerequisites","text":"<ol> <li>Add the KubeAI helm repository.</li> </ol> <pre><code>helm repo add kubeai https://www.kubeai.org\nhelm repo update\n</code></pre> <ol> <li>(Optional) Set the Hugging Face token as an environment variable. This is only required if you plan to use HuggingFace models that require authentication.</li> </ol> <pre><code>export HF_TOKEN=&lt;your-hugging-face-token&gt;\n</code></pre>"},{"location":"installation/any/#installation-using-only-cpus","title":"Installation using only CPUs","text":"<p>All engines supported in KubeAI also support running only on CPU resources.</p> <p>Install KubeAI using the pre-defined values file which defines CPU resourceProfiles:</p> <pre><code>helm install kubeai kubeai/kubeai --wait \\\n  --set secrets.huggingface.token=$HF_TOKEN\n</code></pre> <p>Optionally, inspect the values file to see the default resourceProfiles:</p> <pre><code>helm show values kubeai/kubeai &gt; values.yaml\n</code></pre>"},{"location":"installation/any/#installation-using-nvidia-gpus","title":"Installation using NVIDIA GPUs","text":"<p>This section assumes you have a Kubernetes cluster with NVIDIA GPU resources available and installed the NVIDIA device plugin that adds GPU information labels to the nodes.</p> <p>This time we need to use a custom resource profiles that define the nodeSelectors for different GPU types.</p> <p>Download the values file for the NVIDIA GPU operator:</p> <pre><code>curl -L -O https://raw.githubusercontent.com/substratusai/kubeai/refs/heads/main/charts/kubeai/values-nvidia-k8s-device-plugin.yaml\n</code></pre> <p>You likely will not need to modify the <code>values-nvidia-k8s-device-plugin.yaml</code> file. However, do inspect the file to ensure the GPU resourceProfile nodeSelectors match the node labels on your nodes.</p> <p>Install KubeAI using the custom resourceProfiles: <pre><code>helm upgrade --install kubeai kubeai/kubeai \\\n    -f values-nvidia-k8s-device-plugin.yaml \\\n    --set secrets.huggingface.token=$HF_TOKEN \\\n    --wait\n</code></pre></p>"},{"location":"installation/any/#installation-using-amd-gpus","title":"Installation using AMD GPUs","text":"<p>This section assumes you have a Kubernetes cluster with AMD GPU resources available and installed the AMD device plugin that adds GPU information labels to the nodes.</p> <p>This time we need to use a custom resource profiles that define the nodeSelectors for different GPU types.</p> <p>Download the values file for the AMD GPU operator:</p> <pre><code>curl -L -O https://raw.githubusercontent.com/substratusai/kubeai/refs/heads/main/charts/kubeai/values-amd-gpu-device-plugin.yaml\n</code></pre> <p>You likely will not need to modify the <code>values-amd-gpu-device-plugin.yaml</code> file. However, do inspect the file to ensure the GPU resourceProfile nodeSelectors match the node labels on your nodes.</p> <p>Install KubeAI using the custom resourceProfiles: <pre><code>helm upgrade --install kubeai kubeai/kubeai \\\n    -f values-amd-gpu-device-plugin.yaml \\\n    --set secrets.huggingface.token=$HF_TOKEN \\\n    --wait\n</code></pre></p>"},{"location":"installation/any/#deploying-models","title":"Deploying models","text":"<p>Take a look at the following how-to guides to deploy models:</p> <ul> <li>Configure Text Generation Models</li> <li>Configure Embedding Models</li> <li>Configure Reranking Models</li> <li>Configure Speech to Text Models</li> </ul>"},{"location":"installation/eks/","title":"Install on EKS","text":"TIP: Make sure you have enough GPU quota in your AWS account. <p>The default quotas for GPU instances are often 0. You will need to request a quota increase for the GPU instances you want to use.</p> <p>The following quotas may require an increase if you wish to use GPUs in your EKS cluster: - All G and VT Spot Instance Requests - All P5 Spot Instance Requests - All P4, P3 and P2 Spot Instance Requests - Running Dedicated p4d Hosts</p>"},{"location":"installation/eks/#1-create-eks-cluster-with-karpenter","title":"1. Create EKS cluster with Karpenter","text":"<p>Set the environment variables used throughout this guide:</p> <pre><code>export CLUSTER_NAME=\"cluster-with-karpenter\"\nexport AWS_DEFAULT_REGION=\"us-west-2\"\nexport K8S_VERSION=\"1.30\"\nexport GPU_AMI_ID=\"$(aws ssm get-parameter --name /aws/service/eks/optimized-ami/${K8S_VERSION}/amazon-linux-2-gpu/recommended/image_id --query Parameter.Value --output text)\"\n</code></pre> <p>Create the EKS cluster using eksctl: <pre><code>eksctl create cluster -f - &lt;&lt;EOF\n---\napiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\nmetadata:\n  name: \"${CLUSTER_NAME}\"\n  region: \"${AWS_DEFAULT_REGION}\"\n  version: \"${K8S_VERSION}\"\n  tags:\n    karpenter.sh/discovery: \"${CLUSTER_NAME}\" # here, it is set to the cluster name\n\niam:\n  withOIDC: true # required\n\nkarpenter:\n  version: '1.0.6' # Exact version must be specified\n\nmanagedNodeGroups:\n- instanceType: m5.large\n  amiFamily: AmazonLinux2\n  name: \"${CLUSTER_NAME}-m5-ng\"\n  desiredCapacity: 2\n  minSize: 1\n  maxSize: 10\nEOF\n</code></pre></p>"},{"location":"installation/eks/#2-configure-a-karpenter-gpu-nodepool","title":"2. Configure a Karpenter GPU NodePool","text":"<p>Create the NodePool and EC2NodeClass objects:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: karpenter.sh/v1\nkind: NodePool\nmetadata:\n  name: gpu\nspec:\n  template:\n    spec:\n      requirements:\n        - key: karpenter.sh/capacity-type\n          operator: In\n          values: [\"spot\", \"on-demand\"]\n        - key: karpenter.k8s.aws/instance-category\n          operator: In\n          values: [\"g\", \"p\"]\n      nodeClassRef:\n        group: karpenter.k8s.aws\n        kind: EC2NodeClass\n        name: gpu\n      expireAfter: 720h # 30 * 24h = 720h\n      taints:\n      - key: nvidia.com/gpu\n        value: \"true\"\n        effect: NoSchedule\n  limits:\n    cpu: 1000\n  disruption:\n    consolidationPolicy: WhenEmptyOrUnderutilized\n    consolidateAfter: 1m\n---\napiVersion: karpenter.k8s.aws/v1\nkind: EC2NodeClass\nmetadata:\n  name: gpu\nspec:\n  amiFamily: AL2 # Amazon Linux 2\n  role: \"eksctl-KarpenterNodeRole-${CLUSTER_NAME}\"\n  subnetSelectorTerms:\n    - tags:\n        karpenter.sh/discovery: \"${CLUSTER_NAME}\" # replace with your cluster name\n  securityGroupSelectorTerms:\n    - tags:\n        karpenter.sh/discovery: \"${CLUSTER_NAME}\" # replace with your cluster name\n  amiSelectorTerms:\n    - id: \"${GPU_AMI_ID}\" # &lt;- GPU Optimized AMD AMI \n  blockDeviceMappings:\n    - deviceName: /dev/xvda\n      ebs:\n        volumeSize: 300Gi\n        volumeType: gp3\n        encrypted: true\nEOF\n</code></pre> <p>Install the NVIDIA device plugin (needed for Karpenter nodes):</p> <pre><code>kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.16.1/deployments/static/nvidia-device-plugin.yml\n</code></pre>"},{"location":"installation/eks/#3-install-kubeai","title":"3. Install KubeAI","text":"<p>Add KubeAI Helm repository.</p> <pre><code>helm repo add kubeai https://www.kubeai.org\nhelm repo update\n</code></pre> <p>Make sure you have a HuggingFace Hub token set in your environment (<code>HUGGING_FACE_HUB_TOKEN</code>).</p> <pre><code>export HF_TOKEN=\"replace-with-your-huggingface-token\"\n</code></pre> <p>Install KubeAI with Helm.</p> <pre><code>curl -L -O https://raw.githubusercontent.com/substratusai/kubeai/refs/heads/main/charts/kubeai/values-eks.yaml\n# Please review the values-eks.yaml file and edit the nodeSelectors if needed.\ncat values-eks.yaml\nhelm upgrade --install kubeai kubeai/kubeai \\\n    -f values-eks.yaml \\\n    --set secrets.huggingface.token=$HF_TOKEN \\\n    --wait\n</code></pre>"},{"location":"installation/eks/#4-deploying-models","title":"4. Deploying models","text":"<p>Take a look at the following how-to guides to deploy models:</p> <ul> <li>Configure Text Generation Models</li> <li>Configure Embedding Models</li> <li>Configure Reranking Models</li> <li>Configure Speech to Text Models</li> </ul>"},{"location":"installation/gke/","title":"Install on GKE","text":"TIP: Make sure you have enough quota in your GCP project. <p>Open the cloud console quotas page: https://console.cloud.google.com/iam-admin/quotas. Make sure your project is selected in the top left.</p> <p>You will need to verify that you have enough quota for the accelerators you want to use. Below is table of common quotas you will have to increase depending on your needs.</p> Quota Location Min Value Preemptible TPU v5 Lite Podslice chips <code>&lt;your-region&gt;</code> 8 Preemptible NVIDIA L4 GPUs <code>&lt;your-region&gt;</code> 2 GPUs (all regions) - 2 CPUs (all regions) - 24 <p>See the following screenshot examples of how these quotas appear in the console:</p> <p></p> <p></p> <p></p> <p></p>"},{"location":"installation/gke/#1-create-a-cluster","title":"1. Create a cluster","text":""},{"location":"installation/gke/#option-gke-autopilot","title":"Option: GKE Autopilot","text":"<p>Create an Autopilot cluster (replace <code>us-central1</code> with a region that you have quota).</p> <pre><code>gcloud container clusters create-auto cluster-1 \\\n    --location=us-central1\n</code></pre>"},{"location":"installation/gke/#option-gke-standard","title":"Option: GKE Standard","text":"<p>TODO: Reference gcloud commands for creating a GKE standard cluster.</p>"},{"location":"installation/gke/#2-install-kubeai","title":"2. Install KubeAI","text":"<p>Add KubeAI Helm repository.</p> <pre><code>helm repo add kubeai https://www.kubeai.org\nhelm repo update\n</code></pre> <p>Make sure you have a HuggingFace Hub token set in your environment (<code>HUGGING_FACE_HUB_TOKEN</code>).</p> <p>Install KubeAI with Helm.</p> <pre><code>curl -L -O https://raw.githubusercontent.com/substratusai/kubeai/refs/heads/main/charts/kubeai/values-gke.yaml\nhelm upgrade --install kubeai kubeai/kubeai \\\n    -f values-gke.yaml \\\n    --set secrets.huggingface.token=$HUGGING_FACE_HUB_TOKEN \\\n    --wait\n</code></pre>"},{"location":"installation/gke/#4-deploying-models","title":"4. Deploying models","text":"<p>Take a look at the following how-to guides to deploy models:</p> <ul> <li>Configure Text Generation Models</li> <li>Configure Embedding Models</li> <li>Configure Reranking Models</li> <li>Configure Speech to Text Models</li> </ul>"},{"location":"reference/kubernetes-api/","title":"Kubernetes API","text":""},{"location":"reference/kubernetes-api/#packages","title":"Packages","text":"<ul> <li>kubeai.org/v1</li> </ul>"},{"location":"reference/kubernetes-api/#kubeaiorgv1","title":"kubeai.org/v1","text":"<p>Package v1 contains API Schema definitions for the kubeai v1 API group</p>"},{"location":"reference/kubernetes-api/#resource-types","title":"Resource Types","text":"<ul> <li>Model</li> </ul>"},{"location":"reference/kubernetes-api/#adapter","title":"Adapter","text":"<p>Appears in: - ModelSpec</p> Field Description Default Validation <code>name</code> string Name must be a lowercase string with no spaces. MaxLength: 63 Pattern: <code>^[a-z0-9-]+$</code> Required: {}  <code>url</code> string"},{"location":"reference/kubernetes-api/#file","title":"File","text":"<p>File represents a file to be mounted in the model pod.</p> <p>Appears in: - ModelSpec</p> Field Description Default Validation <code>path</code> string Path where the file should be mounted in the pod.Must be an absolute path. MaxLength: 1024 Required: {}  <code>content</code> string Content of the file to be mounted.Will be injected into a ConfigMap and mounted in the model Pods. MaxLength: 100000 Required: {}"},{"location":"reference/kubernetes-api/#loadbalancing","title":"LoadBalancing","text":"<p>Appears in: - ModelSpec</p> Field Description Default Validation <code>strategy</code> LoadBalancingStrategy LeastLoad Enum: [LeastLoad PrefixHash] Optional: {}  <code>prefixHash</code> PrefixHash {  } Optional: {}"},{"location":"reference/kubernetes-api/#loadbalancingstrategy","title":"LoadBalancingStrategy","text":"<p>Underlying type: string</p> <p>Validation: - Enum: [LeastLoad PrefixHash]</p> <p>Appears in: - LoadBalancing</p> Field Description <code>LeastLoad</code> <code>PrefixHash</code>"},{"location":"reference/kubernetes-api/#model","title":"Model","text":"<p>Model resources define the ML models that will be served by KubeAI.</p> Field Description Default Validation <code>apiVersion</code> string <code>kubeai.org/v1</code> <code>kind</code> string <code>Model</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> ModelSpec <code>status</code> ModelStatus"},{"location":"reference/kubernetes-api/#modelfeature","title":"ModelFeature","text":"<p>Underlying type: string</p> <p>Validation: - Enum: [TextGeneration TextEmbedding Reranking SpeechToText]</p> <p>Appears in: - ModelSpec</p>"},{"location":"reference/kubernetes-api/#modelspec","title":"ModelSpec","text":"<p>ModelSpec defines the desired state of Model.</p> <p>Appears in: - Model</p> Field Description Default Validation <code>url</code> string URL of the model to be served.Currently the following formats are supported:For VLLM, FasterWhisper, Infinity engines:\"hf:///\"\"pvc://\"\"pvc:///\"\"gs:///\" (only with cacheProfile)\"oss:///\" (only with cacheProfile)\"s3:///\" (only with cacheProfile)For OLlama engine:\"ollama://\" Required: {}  <code>adapters</code> Adapter array <code>features</code> ModelFeature array Features that the model supports.Dictates the APIs that are available for the model. Enum: [TextGeneration TextEmbedding Reranking SpeechToText]  <code>engine</code> string Engine to be used for the server process. Enum: [OLlama VLLM FasterWhisper Infinity] Required: {}  <code>resourceProfile</code> string ResourceProfile required to serve the model.Use the format \":\".Example: \"nvidia-gpu-l4:2\" - 2x NVIDIA L4 GPUs.Must be a valid ResourceProfile defined in the system config. <code>cacheProfile</code> string CacheProfile to be used for caching model artifacts.Must be a valid CacheProfile defined in the system config. <code>image</code> string Image to be used for the server process.Will be set from ResourceProfile + Engine if not specified. <code>args</code> string array Args to be added to the server process. <code>env</code> object (keys:string, values:string) Env variables to be added to the server process. <code>envFrom</code> EnvFromSource array Env variables to be added to the server process from Secret or ConfigMap. <code>replicas</code> integer Replicas is the number of Pod replicas that should be activelyserving the model. KubeAI will manage this field unless AutoscalingDisabledis set to true. <code>minReplicas</code> integer MinReplicas is the minimum number of Pod replicas that the model can scale down to.Note: 0 is a valid value. Minimum: 0 Optional: {}  <code>maxReplicas</code> integer MaxReplicas is the maximum number of Pod replicas that the model can scale up to.Empty value means no limit. Minimum: 1  <code>autoscalingDisabled</code> boolean AutoscalingDisabled will stop the controller from managing the replicasfor the Model. When disabled, metrics will not be collected on server Pods. <code>targetRequests</code> integer TargetRequests is average number of active requests that the autoscalerwill try to maintain on model server Pods. 100 Minimum: 1  <code>scaleDownDelaySeconds</code> integer ScaleDownDelay is the minimum time before a deployment is scaled down afterthe autoscaling algorithm determines that it should be scaled down. 30 <code>owner</code> string Owner of the model. Used solely to populate the owner field in theOpenAI /v1/models endpoint.DEPRECATED. Optional: {}  <code>loadBalancing</code> LoadBalancing LoadBalancing configuration for the model.If not specified, a default is used based on the engine and request. {  } <code>files</code> File array Files to be mounted in the model Pods. MaxItems: 10  <code>priorityClassName</code> string PriorityClassName sets the priority class for all pods created for this model.If specified, the PriorityClass must exist before the model is created.This is useful for implementing priority and preemption for models. Optional: {}"},{"location":"reference/kubernetes-api/#modelstatus","title":"ModelStatus","text":"<p>ModelStatus defines the observed state of Model.</p> <p>Appears in: - Model</p> Field Description Default Validation <code>replicas</code> ModelStatusReplicas <code>cache</code> ModelStatusCache"},{"location":"reference/kubernetes-api/#modelstatuscache","title":"ModelStatusCache","text":"<p>Appears in: - ModelStatus</p> Field Description Default Validation <code>loaded</code> boolean"},{"location":"reference/kubernetes-api/#modelstatusreplicas","title":"ModelStatusReplicas","text":"<p>Appears in: - ModelStatus</p> Field Description Default Validation <code>all</code> integer <code>ready</code> integer"},{"location":"reference/kubernetes-api/#prefixhash","title":"PrefixHash","text":"<p>Appears in: - LoadBalancing</p> Field Description Default Validation <code>meanLoadFactor</code> integer MeanLoadPercentage is the percentage that any given endpoint's load must not exceedover the mean load of all endpoints in the hash ring. Defaults to 125% which isa widely accepted value for the Consistent Hashing with Bounded Loads algorithm. 125 Minimum: 100 Optional: {}  <code>replication</code> integer Replication is the number of replicas of each endpoint on the hash ring.Higher values will result in a more even distribution of load but willdecrease lookup performance. 256 Optional: {}  <code>prefixCharLength</code> integer PrefixCharLength is the number of characters to count when building the prefix to hash. 100 Optional: {}"},{"location":"reference/openai-api-compatibility/","title":"OpenAI API Compatibility","text":"<p>KubeAI provides an OpenAI API compatiblity layer.</p>"},{"location":"reference/openai-api-compatibility/#general","title":"General:","text":""},{"location":"reference/openai-api-compatibility/#models","title":"Models","text":"<pre><code>GET /v1/models\n</code></pre> <ul> <li>Lists all <code>kind: Model</code> object installed in teh Kubernetes API Server.</li> </ul>"},{"location":"reference/openai-api-compatibility/#inference","title":"Inference","text":""},{"location":"reference/openai-api-compatibility/#text-generation","title":"Text Generation","text":"<pre><code>POST /v1/chat/completions\nPOST /v1/completions\n</code></pre> <ul> <li>Supported for Models with <code>.spec.features: [\"TextGeneration\"]</code>.</li> </ul>"},{"location":"reference/openai-api-compatibility/#embeddings","title":"Embeddings","text":"<pre><code>POST /v1/embeddings\n</code></pre> <ul> <li>Supported for  Models with <code>.spec.features: [\"TextEmbedding\"]</code>.</li> </ul>"},{"location":"reference/openai-api-compatibility/#reranking","title":"Reranking","text":"<pre><code>POST /v1/vllm/rerank\n</code></pre> <ul> <li>Supported for  Models with <code>.spec.features: [\"Reranking\"]</code>.</li> </ul>"},{"location":"reference/openai-api-compatibility/#speech-to-text","title":"Speech-to-Text","text":"<pre><code>POST /v1/audio/transcriptions\n</code></pre> <ul> <li>Supported for Models with <code>.spec.features: [\"SpeechToText\"]</code>.</li> </ul>"},{"location":"reference/openai-api-compatibility/#openai-client-libaries","title":"OpenAI Client libaries","text":"<p>You can use the official OpenAI client libraries by setting the <code>base_url</code> to the KubeAI endpoint.</p> <p>For example, you can use the Python client like this: <pre><code>from openai import OpenAI\nclient = OpenAI(api_key=\"ignored\",\n                base_url=\"http://kubeai/openai/v1\")\nresponse = client.chat.completions.create(\n  model=\"gemma2-2b-cpu\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n  ]\n)\n</code></pre></p>"},{"location":"tutorials/langchain/","title":"Using LangChain with KubeAI","text":"<p>LangChain makes it easy to build applications powered by LLMs. KubeAI makes it easy to deploy and manage LLMs at scale. Together, they make it easy to build and deploy private and secure AI applications.</p> <p>In this tutorial, we'll show you how to use LangChain with KubeAI's OpenAI compatible API. The beauty of KubeAI's OpenAI compatibility is that you can use KubeAI with any framework that supports OpenAI.</p>"},{"location":"tutorials/langchain/#prerequisites","title":"Prerequisites","text":"<p>A K8s cluster. You can use a local cluster like kind.</p>"},{"location":"tutorials/langchain/#installing-kubeai-with-gemma-2b","title":"Installing KubeAI with Gemma 2B","text":"<p>Run the following command to install KubeAI with Gemma 2B:</p> <pre><code>helm repo add kubeai https://www.kubeai.org\nhelm repo update\n\ncat &lt;&lt;EOF &gt; models-helm-values.yaml\ncatalog:\n  gemma2-2b-cpu:\n    enabled: true\n    minReplicas: 1\nEOF\n\nhelm install kubeai kubeai/kubeai \\\n    -f ./helm-values.yaml \\\n    --wait --timeout 10m\n\nhelm install kubeai-models kubeai/models \\\n    -f ./models-helm-values.yaml\n</code></pre>"},{"location":"tutorials/langchain/#using-langchain","title":"Using LangChain","text":"<p>Install the required Python packages: <pre><code>pip install langchain_openai\n</code></pre></p> <p>Let's access the KubeAI OpenAI compatible API locally to make it easier.</p> <p>Run the following command to port-forward to the KubeAI service: <pre><code>kubectl port-forward svc/kubeai 8000:80\n</code></pre> Now the KubeAI OpenAI compatible API is available at <code>http://localhost:8000/openai</code> from your local machine.</p> <p>Let's create a simple Python script that uses LangChain and is connected to KubeAI.</p> <p>Create a file named <code>test-langchain.py</code> with the following content: <pre><code>from langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    model=\"gemma2-2b-cpu\",\n    temperature=0,\n    max_tokens=None,\n    timeout=None,\n    max_retries=2,\n    api_key=\"thisIsIgnored\",\n    base_url=\"http://localhost:8000/openai/v1\",\n)\n\nmessages = [\n    (\n        \"system\",\n        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n    ),\n    (\"human\", \"I love programming.\"),\n]\nai_msg = llm.invoke(messages)\nprint(ai_msg.content)\n</code></pre></p> <p>Run the Python script: <pre><code>python test-langchain.py\n</code></pre></p> <p>Notice that we set base_url to <code>http://localhost:8000/openai/v1</code>. This tells LangChain to use our local KubeAI OpenAI compatible AP instead of the default OpenAI public API.</p> <p>If you run langchain within the K8s cluster, you can use the following base_url instead: <code>http://kubeai/openai/v1</code>. So the code would look like this: <pre><code>llm = ChatOpenAI(\n    ...\n    base_url=\"http://kubeai/openai/v1\",\n)\n</code></pre></p> <p>That's it! You've successfully used LangChain with KubeAI. Now you can build and deploy private and secure AI applications with ease.</p>"},{"location":"tutorials/langtrace/","title":"Deploying KubeAI with Langtrace","text":"<p>Langtrace is an open source tool that helps you with tracing and monitoring your AI calls. It includes a self-hosted UI that for example shows you the estimated costs of your LLM calls.</p> <p>KubeAI is used for deploying LLMs with an OpenAI compatible endpoint.</p> <p>In this tutorial you will learn how to deploy KubeAI and Langtrace end-to-end. Both KubeAI and Langtrace are installed in your Kubernetes cluster. No cloud services or external dependencies are required.</p> <p>If you don't have a K8s cluster yet, you can create one using kind or minikube. <pre><code>kind create cluster # OR: minikube start\n</code></pre></p> <p>Install Langtrace: <pre><code>helm repo add langtrace https://Scale3-Labs.github.io/langtrace-helm-chart\nhelm repo update\nhelm install langtrace langtrace/langtrace\n</code></pre></p> <p>Install KubeAI and wait for all components to be ready (may take a minute). <pre><code>helm repo add kubeai https://www.kubeai.org\nhelm repo update\nhelm install kubeai kubeai/kubeai --wait --timeout 10m\n</code></pre></p> <p>Install the gemma2-2b-cpu model:</p> <pre><code>cat &lt;&lt;EOF &gt; kubeai-models.yaml\ncatalog:\n  gemma2-2b-cpu:\n    enabled: true\n    minReplicas: 1\nEOF\n\nhelm install kubeai-models kubeai/models \\\n    -f ./kubeai-models.yaml\n</code></pre> <p>Create a local Python environment and install dependencies: <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\npip install langtrace-python-sdk openai\n</code></pre></p> <p>Expose the KubeAI service to your local port: <pre><code>kubectl port-forward service/kubeai 8000:80\n</code></pre></p> <p>Expose the Langtrace service to your local port: <pre><code>kubectl port-forward service/langtrace-app 3000:3000\n</code></pre></p> <p>A Langtrace API key is required to use the Langtrace SDK. So lets get one by visiting your self hosted Langtace UI.</p> <p>Open your browser to http://localhost:3000, create a project and get the API keys for your langtrace project.</p> <p>In the Python script below, replace <code>langtrace_api_key</code> with your API key.</p> <p>Create file named <code>langtrace-example.py</code> with the following content: <pre><code># Replace this with your langtrace API key by visiting http://localhost:3000\nlangtrace_api_key=\"f7e003de19b9a628258531c17c264002e985604ca9fa561debcc85c41f357b09\"\n\nfrom langtrace_python_sdk import langtrace\nfrom langtrace_python_sdk.utils.with_root_span import with_langtrace_root_span\n# Paste this code after your langtrace init function\n\nfrom openai import OpenAI\n\nlangtrace.init(\n    api_key=api_key,\n    api_host=\"http://localhost:3000/api/trace\",\n)\n\nbase_url = \"http://localhost:8000/openai/v1\"\nmodel = \"gemma2-2b-cpu\"\n\n@with_langtrace_root_span()\ndef example():\n    client = OpenAI(base_url=base_url, api_key=\"ignored-by-kubeai\")\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"How many states of matter are there?\"\n            }\n        ],\n    )\n    print(response.choices[0].message.content)\n\nexample()\n</code></pre></p> <p>Run the Python script: <pre><code>python3 langtrace-example.py\n</code></pre></p> <p>Now you should see the trace in your Langtrace UI. Take a look by visiting http://localhost:3000.</p> <p></p>"},{"location":"tutorials/private-deep-chat/","title":"Private Deep Chat","text":"<p>In this tutorial you are going to deploy a custom, multitenant, private chat application. The Chat UI is powered by Deep Chat - an open source web component that is easy to embed into any frontend web app framework or simple HTML page. KubeAI, which serves LLMs on Kubernetes, will be used to ensure that chat interactions are not sent out of the cluster.</p> <p></p> <p>In this example, we will deploy a custom Go server that will authenticate users using Basic Authentication. When a webpage is requested, a simple HTML page with the <code>&lt;deep-chat&gt;</code> web component will be served. We will configure Deep Chat and KubeAI to communicate using the OpenAI API format:</p> <pre><code>&lt;deep-chat\n    connect='{\"url\":\"/openai/v1/chat/completions\", ... }'\n    directConnection='{\"openAI\": ... }'&gt;\n&lt;/deep-chat&gt;\n</code></pre> <p>When the HTML page loads we will use Javascript to make an initial request to fetch available models. The Go server will proxy this request to KubeAI:</p> <pre><code>proxyHandler := httputil.NewSingleHostReverseProxy(kubeAIURL)\n\nhttp.Handle(\"/openai/\", authUserToKubeAI(proxyHandler))\n</code></pre> <p>The server will translate the username and password provided in the basic auth header into a label selector that will tell KubeAI to filter the models it returns. The same approach will also be used to enforce access at inference-time.</p> <pre><code>r.Header.Set(\"X-Label-Selector\", fmt.Sprintf(\"tenancy in (%s)\",\n    strings.Join(tenancy, \",\"),\n))\n</code></pre> <p>While this is a simple example application, this overall architecture can be used when incorporating chat into a production application.</p> <p></p>"},{"location":"tutorials/private-deep-chat/#guide","title":"Guide","text":"<p>Create a local cluster with kind and install KubeAI.</p> <pre><code>kind create cluster\n\nhelm repo add kubeai https://www.kubeai.org &amp;&amp; helm repo update\nhelm install kubeai kubeai/kubeai --set open-webui.enabled=false --wait --timeout 5m\n</code></pre> <p>Clone the KubeAI repo and navigate to the example directory.</p> <pre><code>git clone https://github.com/substratusai/kubeai\ncd ./kubeai/examples/private-deep-chat\n</code></pre> <p>Build the private chat application and load the image into the local kind cluster.</p> <pre><code>docker build -t private-deep-chat:latest .\nkind load docker-image private-deep-chat:latest\n</code></pre> <p>Deploy the private chat application along with some KubeAI Models.</p> <pre><code>kubectl apply -f ./manifests\n</code></pre> <p>Start a port-forward.</p> <pre><code>kubectl port-forward svc/private-deep-chat 8000:80\n</code></pre> <p>In your browser, navigate to localhost:8000.</p> <p>NOTE: Your first interaction with a given model may take a minute as the model needs to scale from zero. Use <code>kubectl get pods</code> to check the scale up progress.</p> <p>Login as any of the following users:</p> User Password nick nickspass sam samspass joe joespass <p>These users each have access to different KubeAI Models. You can see this assignment by looking at the user mapping in <code>./main.go</code> and the associated <code>tenancy</code> label on the Models in <code>./manifests/models.yaml</code>.</p>"},{"location":"tutorials/weaviate/","title":"Weaviate with local autoscaling embedding and generative models","text":"<p>Weaviate is a vector search engine that can integrate seamlessly with KubeAI's embedding and generative models. This tutorial demonstrates how to deploy both KubeAI and Weaviate in a Kubernetes cluster, using KubeAI as the OpenAI endpoint for Weaviate.</p> <p>Why use KubeAI with Weaviate?</p> <ul> <li>Security and privacy: KubeAI runs locally in your Kubernetes cluster, so your data never leaves your infrastructure.</li> <li>Cost savings: KubeAI can run on your existing hardware, reducing the need for paying for embeddings and generative models.</li> </ul> <p>This tutorial uses CPU only models, so it should work even on your laptop.</p> <p>As you go go through this tutorial, you will learn how to:</p> <ul> <li>Deploy KubeAI with embedding and generative models</li> <li>Install Weaviate and connect it to KubeAI</li> <li>Import data into Weaviate</li> <li>Perform semantic search using the embedding model</li> <li>Perform generative search using the generative model</li> </ul>"},{"location":"tutorials/weaviate/#prerequisites","title":"Prerequisites","text":"<p>A Kubernetes cluster. You can use kind or minikube.</p> <pre><code>kind create cluster\n</code></pre>"},{"location":"tutorials/weaviate/#kubeai-configuration","title":"KubeAI Configuration","text":"<p>Let's start by deploying KubeAI with the models we want to use. Nomic embedding model is used instead of text-embedding-ada-002. Gemma 2 2B is used instead of gpt-3.5-turbo. You could choose to use bigger models depending on your available hardware.</p> <p>Create a file named <code>kubeai-model-values.yaml</code> with the following content: <pre><code>catalog:\n  text-embedding-ada-002:\n    enabled: true\n    minReplicas: 1\n    features: [\"TextEmbedding\"]\n    owner: nomic\n    url: \"ollama://nomic-embed-text\"\n    engine: OLlama\n    resourceProfile: cpu:1\n  gpt-3.5-turbo:\n    enabled: true\n    minReplicas: 1\n    features: [\"TextGeneration\"]\n    owner: google\n    url: \"ollama://gemma2:2b\"\n    engine: OLlama\n    resourceProfile: cpu:2\n</code></pre></p> <p>Note: It's important that you name the models as <code>text-embedding-ada-002</code> and <code>gpt-3.5-turbo</code> as Weaviate expects these names.</p> <p>Run the following command to deploy KubeAI and install the configured models: <pre><code>helm repo add kubeai https://www.kubeai.org &amp;&amp; helm repo update\n\nhelm install kubeai kubeai/kubeai\n\nhelm install kubeai-models kubeai/models \\\n    -f ./kubeai-model-values.yaml\n</code></pre></p>"},{"location":"tutorials/weaviate/#weaviate-installation","title":"Weaviate Installation","text":"<p>For this tutorial, we will use the Weaviate Helm chart to deploy Weaviate.</p> <p>Let's enable the text2vec-openai and generative-openai modules in Weaviate. We will also set the default vectorizer module to text2vec-openai.</p> <p>The <code>apiKey</code> is ignored in this case as we are using KubeAI as the OpenAI endpoint.</p> <p>Create a file named <code>weaviate-values.yaml</code> with the following content: <pre><code>modules:\n  text2vec-openai:\n    enabled: true\n    apiKey: thisIsIgnored\n  generative-openai:\n    enabled: true\n    apiKey: thisIsIgnored\n  default_vectorizer_module: text2vec-openai\nservice:\n  # To prevent Weaviate being exposed publicly\n  type: ClusterIP\n</code></pre></p> <p>Install Weaviate by running the following command: <pre><code>helm repo add weaviate https://weaviate.github.io/weaviate-helm &amp;&amp; helm repo update\n\nhelm install \\\n  \"weaviate\" \\\n  weaviate/weaviate \\\n  -f weaviate-values.yaml\n</code></pre></p>"},{"location":"tutorials/weaviate/#usage","title":"Usage","text":"<p>We will be using Python to interact with Weaviate. The 2 use cases we will cover are: - Semantic search using the embedding model - Generative search using the generative model</p>"},{"location":"tutorials/weaviate/#connectivity","title":"Connectivity","text":"<p>The remaining steps require connectivity to the Weaviate service. However, Weaviate is not exposed publicly in this setup. So we setup a local port forwards to access the Weaviate services.</p> <p>Setup a local port forwards to the Weaviate services by running: <pre><code>kubectl port-forward svc/weaviate 8080:80\nkubectl port-forward svc/weaviate-grpc 50051:50051\n</code></pre></p>"},{"location":"tutorials/weaviate/#weaviate-client-python-setup","title":"Weaviate client Python Setup","text":"<p>Create a virtual environment and install the Weaviate client: <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -U weaviate-client requests\n</code></pre></p>"},{"location":"tutorials/weaviate/#collection-and-data-import","title":"Collection and Data Import","text":"<p>Create a file named <code>create-collection.py</code> with the following content: <pre><code>import json\nimport weaviate\nimport requests\nfrom weaviate.classes.config import Configure\n\n# This works due to port forward in previous step\nwith weaviate.connect_to_local(port=8080, grpc_port=50051) as client:\n\n    client.collections.create(\n        \"Question\",\n        vectorizer_config=Configure.Vectorizer.text2vec_openai(\n                model=\"text-embedding-ada-002\",\n                base_url=\"http://kubeai/openai\",\n        ),\n        generative_config=Configure.Generative.openai(\n            model=\"gpt-3.5-turbo\",\n            base_url=\"http://kubeai/openai\",\n        ),\n    )\n\n    # import data\n    resp = requests.get('https://raw.githubusercontent.com/weaviate-tutorials/quickstart/main/data/jeopardy_tiny.json')\n    data = json.loads(resp.text)  # Load data\n\n    question_objs = list()\n    for i, d in enumerate(data):\n        question_objs.append({\n            \"answer\": d[\"Answer\"],\n            \"question\": d[\"Question\"],\n            \"category\": d[\"Category\"],\n        })\n\n    questions = client.collections.get(\"Question\")\n    questions.data.insert_many(question_objs)\n    print(\"Data imported successfully\")\n</code></pre></p> <p>Create a collection that uses KubeAI as the openAI endpoint: <pre><code>python create-collection.py\n</code></pre> You should see a message <code>Data imported successfully</code>.</p> <p>The collection is now created and data is imported. The vectors are generated by KubeAI and stored in Weaviate.</p>"},{"location":"tutorials/weaviate/#semantic-search","title":"Semantic Search","text":"<p>Now let's do semantic search, which uses the embeddings. Create a file named <code>search.py</code> with the following content: <pre><code>import weaviate\nfrom weaviate.classes.config import Configure\n\n# This works due to port forward in previous step\nwith weaviate.connect_to_local(port=8080, grpc_port=50051) as client:\n    questions = client.collections.get(\"Question\")\n    response = questions.query.near_text(\n        query=\"biology\",\n        limit=2\n    )\n    print(response.objects[0].properties)  # Inspect the first object\n</code></pre></p> <p>Execute the python script: <pre><code>python search.py\n</code></pre></p> <p>You should see the following output: <pre><code>{\n  \"answer\": \"DNA\",\n  \"question\": \"In 1953 Watson &amp; Crick built a model of the molecular structure of this, the gene-carrying substance\",\n  \"category\": \"SCIENCE\"\n}\n</code></pre></p>"},{"location":"tutorials/weaviate/#generative-search-rag","title":"Generative Search (RAG)","text":"<p>Now let's do generative search, which uses the generative model (Text generation LLM). The generative model is run locally and managed by KubeAI.</p> <p>Create a file named <code>generate.py</code> with the following content: <pre><code>import weaviate\nfrom weaviate.classes.config import Configure\n\n# This works due to port forward in previous step\nwith weaviate.connect_to_local(port=8080, grpc_port=50051) as client:\n    questions = client.collections.get(\"Question\")\n\n    response = questions.generate.near_text(\n        query=\"biology\",\n        limit=2,\n        grouped_task=\"Write a tweet with emojis about these facts.\"\n    )\n\n    print(response.generated)  # Inspect the generated text\n</code></pre></p> <p>Run the python script: <pre><code>python generate.py\n</code></pre></p> <p>You should see something similar to this:</p> <p>\ud83e\uddec Watson &amp; Crick cracked the code in 1953!  \ud83e\udd2f They built a model of DNA, the blueprint of life. \ud83e\uddec \ud83e\udde0 Liver power! \ud83d\udcaa This organ keeps your blood sugar balanced by storing glucose as glycogen. \ud83e\ude78 #ScienceFacts #Biology</p>"},{"location":"tutorials/weaviate/#conclusion","title":"Conclusion","text":"<p>You've now successfully set up KubeAI with Weaviate for both embedding-based semantic search and generative tasks. You've also learned how to import data, perform searches, and generate content using KubeAI-managed models.</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/performance/","title":"Performance","text":""}]}